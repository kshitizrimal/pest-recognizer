{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-cLW0h7DF68"
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "SjC6MoarNTo-",
    "outputId": "7313b3b7-a853-4140-cdb5-6ccfae62b295"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n",
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import zipfile\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "  \n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K18U74lF56gH"
   },
   "source": [
    "#### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1m3zGNn_58jC"
   },
   "outputs": [],
   "source": [
    "classes = ['rice_leaf_roller',\n",
    " 'rice_leaf_caterpillar',\n",
    " 'paddy_stem_maggot',\n",
    " 'asiatic_rice_borer',\n",
    " 'yellow_rice_borer',\n",
    " 'rice_gall_midge',\n",
    " 'rice_stemfly',\n",
    " 'brown_plant_hopper',\n",
    " 'white_backed_plant_hopper',\n",
    " 'small_brown_plant_hopper',\n",
    " 'rice_water_weevil',\n",
    " 'rice_leafhopper',\n",
    " 'grain_spreader_thrips',\n",
    " 'rice_shell_pest',\n",
    " 'grub',\n",
    " 'mole_cricket',\n",
    " 'wireworm',\n",
    " 'white_margined_moth',\n",
    " 'black_cutworm',\n",
    " 'large_cutworm',\n",
    " 'yellow_cutworm',\n",
    " 'red_spider',\n",
    " 'corn_borer',\n",
    " 'army_worm',\n",
    " 'aphids',\n",
    " 'potosiabre_vitarsis',\n",
    " 'peach_borer',\n",
    " 'english_grain_aphid',\n",
    " 'green_bug',\n",
    " 'bird_cherry-oataphid',\n",
    " 'wheat_blossom_midge',\n",
    " 'penthaleus_major',\n",
    " 'longlegged_spider_mite',\n",
    " 'wheat_phloeothrips',\n",
    " 'wheat_sawfly',\n",
    " 'cerodonta_denticornis',\n",
    " 'beet_fly',\n",
    " 'flea_beetle',\n",
    " 'cabbage_army_worm',\n",
    " 'beet_army_worm',\n",
    " 'beet_spot_flies',\n",
    " 'meadow_moth',\n",
    " 'beet_weevil',\n",
    " 'sericaorient_alismots_chulsky',\n",
    " 'alfalfa_weevil',\n",
    " 'flax_budworm',\n",
    " 'alfalfa_plant_bug',\n",
    " 'tarnished_plant_bug',\n",
    " 'locustoidea',\n",
    " 'lytta_polita',\n",
    " 'legume_blister_beetle',\n",
    " 'blister_beetle',\n",
    " 'therioaphis_maculata_buckton',\n",
    " 'odontothrips_loti',\n",
    " 'thrips',\n",
    " 'alfalfa_seed_chalcid',\n",
    " 'pieris_canidia',\n",
    " 'apolygus_lucorum',\n",
    " 'limacodidae',\n",
    " 'viteus_vitifoliae',\n",
    " 'colomerus_vitis',\n",
    " 'brevipoalpus_lewisi_mcgregor',\n",
    " 'oides_decempunctata',\n",
    " 'polyphagotars_onemus_latus',\n",
    " 'pseudococcus_comstocki_kuwana',\n",
    " 'parathrene_regalis',\n",
    " 'ampelophaga',\n",
    " 'lycorma_delicatula',\n",
    " 'xylotrechus',\n",
    " 'cicadella_viridis',\n",
    " 'miridae',\n",
    " 'trialeurodes_vaporariorum',\n",
    " 'erythroneura_apicalis',\n",
    " 'papilio_xuthus',\n",
    " 'panonchus_citri_mcgregor',\n",
    " 'phyllocoptes_oleiverus_ashmead',\n",
    " 'icerya_purchasi_maskell',\n",
    " 'unaspis_yanonensis',\n",
    " 'ceroplastes_rubens',\n",
    " 'chrysomphalus_aonidum',\n",
    " 'parlatoria_zizyphus_lucus',\n",
    " 'nipaecoccus_vastalor',\n",
    " 'aleurocanthus_spiniferus',\n",
    " 'tetradacus_c_bactrocera_minax',\n",
    " 'dacus_dorsalis(hendel)',\n",
    " 'bactrocera_tsuneonis',\n",
    " 'prodenia_litura',\n",
    " 'adristyrannus',\n",
    " 'phyllocnistis_citrella_stainton',\n",
    " 'toxoptera_citricidus',\n",
    " 'toxoptera_aurantii',\n",
    " 'aphis_citricola_vander_goot',\n",
    " 'scirtothrips_dorsalis_hood',\n",
    " 'dasineura_sp',\n",
    " 'lawana_imitata_melichar',\n",
    " 'salurnis_marginella_guerr',\n",
    " 'deporaus_marginatus_pascoe',\n",
    " 'chlumetia_transversa',\n",
    " 'mango_flat_beak_leafhopper',\n",
    " 'rhytidodera_bowrinii_white',\n",
    " 'sternochetus_frigidus',\n",
    " 'cicadellidae']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZ1jCPOT2GQY"
   },
   "outputs": [],
   "source": [
    "img_shape = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAhNaTtw59sp"
   },
   "source": [
    "#### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FI2tPilFPHIp"
   },
   "outputs": [],
   "source": [
    "def zipdir(path, ziph):\n",
    "    # ziph is zipfile handle\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqBvO8UXjTNr"
   },
   "outputs": [],
   "source": [
    "def create_model_1():\n",
    "  return tf.keras.Sequential(\n",
    "                              [tf.keras.layers.Conv2D(16, 3, padding='same', input_shape=(img_shape,img_shape,3), activation='relu'),\n",
    "                              tf.keras.layers.MaxPooling2D(),\n",
    "                              tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "                              tf.keras.layers.MaxPooling2D(),\n",
    "                              tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "                              tf.keras.layers.Flatten(),\n",
    "                              tf.keras.layers.Dense(128, activation='relu'),\n",
    "                              tf.keras.layers.Dense(102, activation='softmax')])\n",
    "def create_model_2():\n",
    "  return tf.keras.Sequential(\n",
    "                              [tf.keras.layers.Conv2D(64, 3, padding='same', input_shape=(img_shape,img_shape,3), activation='relu'),\n",
    "                              tf.keras.layers.MaxPooling2D(),\n",
    "                              tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "                              tf.keras.layers.MaxPooling2D(),\n",
    "                              tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "                              tf.keras.layers.Conv2D(512, 3, padding='same', activation='relu'),\n",
    "                              tf.keras.layers.Flatten(),\n",
    "                              tf.keras.layers.Dense(128, activation='relu'),\n",
    "                              tf.keras.layers.Dense(102, activation='softmax')])\n",
    "  \n",
    "def create_model_3():\n",
    "  return tf.keras.Sequential(\n",
    "                              [tf.keras.layers.Conv2D(64, 3, padding='same', input_shape=(img_shape,img_shape,3), activation='relu'),\n",
    "                              tf.keras.layers.MaxPooling2D(),\n",
    "                              tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "                              tf.keras.layers.MaxPooling2D(),\n",
    "                              tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "                              tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "                              tf.keras.layers.Flatten(),\n",
    "                              tf.keras.layers.Dense(256, activation='relu'),\n",
    "                              tf.keras.layers.Dense(102, activation='softmax')])\n",
    "  \n",
    "def create_model_4():\n",
    "  return tf.keras.Sequential(\n",
    "                              [tf.keras.layers.Conv2D(128, 3, padding='same', input_shape=(img_shape,img_shape,3), activation='relu'),\n",
    "                              tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "                              tf.keras.layers.MaxPooling2D(),\n",
    "                              tf.keras.layers.Dropout(0.1),\n",
    "                              tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "                              tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "                              tf.keras.layers.Dropout(0.2),\n",
    "                              tf.keras.layers.MaxPooling2D(),\n",
    "                              tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "                              tf.keras.layers.Dropout(0.3),\n",
    "                              tf.keras.layers.Flatten(),\n",
    "                              tf.keras.layers.Dense(128, activation='relu'),\n",
    "                              tf.keras.layers.Dropout(0.2),\n",
    "                              tf.keras.layers.Dense(102, activation='softmax')])\n",
    "\n",
    "def create_model_5():\n",
    "  pre_model = tf.keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=(img_shape,img_shape,3))\n",
    "  return tf.keras.Sequential([pre_model,\n",
    "                             tf.keras.layers.Flatten(),\n",
    "                             tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "                             tf.keras.layers.Dropout(0.3),\n",
    "                             tf.keras.layers.Dense(102, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bML4jZ2XjXVB"
   },
   "outputs": [],
   "source": [
    "def return_location(filename):\n",
    "  return filename\n",
    "  \n",
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  # The second to last is the class-directory\n",
    "  return parts[-2] == classes\n",
    "\n",
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_jpeg(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # resize the image to the desired size.\n",
    "  return tf.image.resize(img, [img_shape, img_shape])\n",
    "\n",
    "def process_path(file_path):\n",
    "  label = get_label(file_path)\n",
    "  # load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  img = decode_img(img)\n",
    "  return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99PgKvbOZFGo"
   },
   "outputs": [],
   "source": [
    "def return_classes(ds_type, locations):\n",
    "\n",
    "  filenames_list = []\n",
    "  classes_temp = []\n",
    "\n",
    "  if(ds_type == 'train'):\n",
    "    image_count = len(tf.io.gfile.glob(train_GCS_FILE_PATTERN))\n",
    "  elif(ds_type == 'test'):\n",
    "    image_count = len(tf.io.gfile.glob(test_GCS_FILE_PATTERN))\n",
    "  else:\n",
    "    image_count = len(tf.io.gfile.glob(val_GCS_FILE_PATTERN))\n",
    "  \n",
    "  for file in locations.take(image_count):\n",
    "   filenames_list.append(file.numpy())\n",
    "\n",
    "  for files in filenames_list:\n",
    "      strfl = files.decode(\"utf-8\") \n",
    "      cls = strfl.split(\"/\")[4]\n",
    "      classes_temp.append(cls)\n",
    "\n",
    "  classes_temp = list(set(classes_temp))\n",
    "  num_classes = len(classes_temp)\n",
    "\n",
    "  return num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQ_7CFzbXygs"
   },
   "outputs": [],
   "source": [
    "def get_classes(train_list, test_list, val_list):\n",
    "  train_locations = train_list.map(return_location)\n",
    "  test_locations = test_list.map(return_location)\n",
    "  val_locations = val_list.map(return_location)\n",
    "\n",
    "  train_classes = return_classes('train', train_locations)\n",
    "  test_classes = return_classes('test', test_locations)\n",
    "  val_classes = return_classes('val', val_locations)\n",
    "  \n",
    "  print(\"Train classes:\", train_classes)\n",
    "  print(\"Test classes:\", test_classes)\n",
    "  print(\"Val classes:\", val_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sv35BnARWQbs"
   },
   "outputs": [],
   "source": [
    "def create_ds():\n",
    "  train_0_ds = tf.data.Dataset.list_files(train_GCS_FILE_PATTERN)\n",
    "  val_0_ds = tf.data.Dataset.list_files(val_GCS_FILE_PATTERN)\n",
    "  test_0_ds = tf.data.Dataset.list_files(test_GCS_FILE_PATTERN)\n",
    "\n",
    "  get_classes(train_0_ds, test_0_ds, val_0_ds)\n",
    "\n",
    "  full_ds = train_0_ds.concatenate(val_0_ds)\n",
    "  full_ds = full_ds.concatenate(test_0_ds)\n",
    "  full_ds = full_ds.shuffle(buffer_size=1000)\n",
    "\n",
    "  dataset_size = tf.data.experimental.cardinality(full_ds).numpy()\n",
    "\n",
    "  full_ds = full_ds.take(0.6*dataset_size)\n",
    "  new_size = tf.data.experimental.cardinality(full_ds).numpy()\n",
    "  \n",
    "  print(\"main full dataset size:\", dataset_size)\n",
    "  print(\"new dataset size:\", new_size)\n",
    "\n",
    "  return full_ds, new_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbRDgsVjQl3W"
   },
   "outputs": [],
   "source": [
    "def data_split(dataset, ds_size, choice = 1):\n",
    "  if (choice == 1):\n",
    "    train_size = int(0.4 * ds_size)\n",
    "    val_size = int(0.2 * ds_size)\n",
    "    test_size = int(0.4 * ds_size)\n",
    "  elif(choice == 2):\n",
    "    train_size = int(0.5 * ds_size)\n",
    "    val_size = int(0.2 * ds_size)\n",
    "    test_size = int(0.3 * ds_size)\n",
    "  elif(choice == 3):\n",
    "    train_size = int(0.6 * ds_size)\n",
    "    val_size = int(0.1 * ds_size)\n",
    "    test_size = int(0.3 * ds_size)\n",
    "  elif(choice == 4):\n",
    "    train_size = int(0.65 * ds_size)\n",
    "    val_size = int(0.15 * ds_size)\n",
    "    test_size = int(0.2 * ds_size)\n",
    "  elif(choice == 5):\n",
    "    train_size = int(0.7 * ds_size)\n",
    "    val_size = int(0.2 * ds_size)\n",
    "    test_size = int(0.1 * ds_size)\n",
    "  \n",
    "  train_ds = dataset.take(train_size)\n",
    "  test_ds = dataset.skip(train_size)\n",
    "  val_ds = test_ds.take(val_size)\n",
    "  test_ds = test_ds.skip(val_size)\n",
    "\n",
    "  train_ds = train_ds.map(process_path)\n",
    "  test_ds = test_ds.map(process_path)\n",
    "  val_ds = val_ds.map(process_path)\n",
    "\n",
    "  train_size = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "  test_size = tf.data.experimental.cardinality(test_ds).numpy()\n",
    "  val_size = tf.data.experimental.cardinality(val_ds).numpy()\n",
    "  \n",
    "  print(train_size, test_size, val_size)\n",
    "  print(\"total: \", train_size+test_size+val_size)\n",
    " \n",
    "  return train_ds, val_ds, test_ds, train_size, test_size, val_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZ9jpiu0c7Gm"
   },
   "source": [
    "#### Connect GStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zrv7Zgcfc9Ts"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "from google.cloud import storage\n",
    "import pickle\n",
    "\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ck3tWh8-c96J"
   },
   "outputs": [],
   "source": [
    "project_id = '###-project-name'\n",
    "bucket_name = 'bucket-name'\n",
    "\n",
    "# !gcloud config set project {project_id}\n",
    "# !gsutil cp  ./test gs://{bucket_name}/test\n",
    "\n",
    "storage_client = storage.Client(project_id)\n",
    "\n",
    "def upload_blob(source_file_name, data_split_num):\n",
    "  bucket_name = 'bucketname'\n",
    "  storage_client = storage.Client('project-name')\n",
    "  bucket = storage_client.bucket(bucket_name)\n",
    "  blob = bucket.blob('training_logs/data_split_'+data_split_num+'/'+source_file_name)\n",
    "\n",
    "  blob.upload_from_filename(source_file_name)\n",
    "  print(\"File {} uploaded to Google Bucket: training_logs/data_split_{}/{}.\".format(source_file_name, data_split_num, source_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7q-jxnOeGqW"
   },
   "outputs": [],
   "source": [
    "# upload_blob(\"adc.json\", \"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhU_9diwDIpb"
   },
   "source": [
    "#### Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8p5xYMYOVVg"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# val_GCS_FILE_PATTERN = 'gs://bucketname/val.zip/val/*/*.jpg'\n",
    "# train_GCS_FILE_PATTERN = 'gs://bucketname/pest_datasets/train/*/*.jpg'\n",
    "# test_GCS_FILE_PATTERN = 'gs://bucketname/pest_datasets/test/*/*.jpg'\n",
    "\n",
    "train_GCS_FILE_PATTERN = '/content/datasets_pest/train/*/*.jpg'\n",
    "test_GCS_FILE_PATTERN = '/content/datasets_pest/test/*/*.jpg'\n",
    "val_GCS_FILE_PATTERN = '/content/datasets_pest/val/*/*.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsRbeqtwDCIV"
   },
   "source": [
    "#### Perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "sbmJ99JXTuXm",
    "outputId": "0acde8f5-d0e8-46c2-bdc5-21e549f776d4"
   },
   "outputs": [],
   "source": [
    "train_url = 'https://storage.googleapis.com/bucketname/train.zip'\n",
    "test_url = 'https://storage.googleapis.com/bucketname/test.zip'\n",
    "val_url = 'https://storage.googleapis.com/bucketname/val.zip'\n",
    "\n",
    "path_to_train_zip = tf.keras.utils.get_file('train.zip', origin=train_url, extract=True, cache_dir='/content/', cache_subdir='datasets_pest')\n",
    "path_to_test_zip = tf.keras.utils.get_file('test.zip', origin=test_url, extract=True, cache_dir='/content/', cache_subdir='datasets_pest')\n",
    "path_to_val_zip = tf.keras.utils.get_file('val.zip', origin=val_url, extract=True, cache_dir='/content/', cache_subdir='datasets_pest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "XJKPb1DG6N9U",
    "outputId": "e577f91b-99ac-42c9-d633-b69d1241ccc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classes: 102\n",
      "Test classes: 102\n",
      "Val classes: 102\n",
      "main full dataset size: 75222\n",
      "new dataset size: 45133\n"
     ]
    }
   ],
   "source": [
    "full_ds, dataset_size = create_ds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmvcYTl2b5z4"
   },
   "source": [
    "## Data Split: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KH80DPjof-9B"
   },
   "outputs": [],
   "source": [
    "choice = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "VnG5rBDc6Y-Y",
    "outputId": "5c924486-4ddd-447f-d43d-819b1727d08c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18053 18054 9026\n",
      "total:  45133\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds, train_size, test_size, val_size = data_split(full_ds, dataset_size, choice=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8gwJt2qDlIz"
   },
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5OgAAlM9Sxt"
   },
   "outputs": [],
   "source": [
    "hypers = [{\"batch\": 25,\"steps\":np.ceil(train_size/25), \"val_steps\": np.ceil(val_size/25),\"optimizer\": tf.optimizers.SGD(learning_rate=0.01),\"epochs\": 10},\n",
    "                {\"batch\": 28,\"steps\":np.ceil(train_size/28), \"val_steps\": np.ceil(val_size/28), \"optimizer\": tf.optimizers.RMSprop(learning_rate=0.001),\"epochs\": 10},\n",
    "                {\"batch\": 30,\"steps\":np.ceil(train_size/30), \"val_steps\": np.ceil(val_size/30), \"optimizer\": tf.optimizers.Adagrad(learning_rate=0.0001),\"epochs\": 10},\n",
    "                {\"batch\": 32,\"steps\":np.ceil(train_size/32), \"val_steps\": np.ceil(val_size/32), \"optimizer\": tf.optimizers.Adam(learning_rate=0.00001),\"epochs\": 10},\n",
    "                {\"batch\": 35,\"steps\":np.ceil(train_size/35), \"val_steps\": np.ceil(val_size/35), \"optimizer\": tf.optimizers.Adam(learning_rate=0.0001),\"epochs\": 10}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnpsYz6zJIr2"
   },
   "source": [
    "#### Prep for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VVOHtTWnI5jD",
    "outputId": "6bc0ec42-bc68-4e68-ebd8-70885f71dd28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 150, 150, 128)     3584      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 150, 150, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 75, 75, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 75, 75, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 37, 37, 128)       295040    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 175232)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               22429824  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 102)               13158     \n",
      "=================================================================\n",
      "Total params: 23,774,438\n",
      "Trainable params: 23,774,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " \n",
      " ------------------------------------------------\n",
      "\n",
      " Training for hyperparameter 3 and model 3 .... \n",
      " \n",
      "\n",
      " train size: 18053, validation size: 9026, batch size: 32, epochs: 10 \n",
      "\n",
      "------------------------------------------------ \n",
      " \n",
      "\n",
      "Train for 565.0 steps, validate for 283.0 steps\n",
      "Epoch 1/10\n",
      "565/565 [==============================] - 196s 346ms/step - loss: 4.3664 - accuracy: 0.0722 - categorical_accuracy: 0.0722 - val_loss: 4.3036 - val_accuracy: 0.0923 - val_categorical_accuracy: 0.0923\n",
      "Epoch 2/10\n",
      "565/565 [==============================] - 95s 168ms/step - loss: 4.2122 - accuracy: 0.1072 - categorical_accuracy: 0.1072 - val_loss: 4.1294 - val_accuracy: 0.1164 - val_categorical_accuracy: 0.1164\n",
      "Epoch 3/10\n",
      "565/565 [==============================] - 95s 168ms/step - loss: 4.1046 - accuracy: 0.1185 - categorical_accuracy: 0.1185 - val_loss: 4.0493 - val_accuracy: 0.1299 - val_categorical_accuracy: 0.1299\n",
      "Epoch 4/10\n",
      "565/565 [==============================] - 95s 169ms/step - loss: 4.0073 - accuracy: 0.1321 - categorical_accuracy: 0.1321 - val_loss: 3.9682 - val_accuracy: 0.1451 - val_categorical_accuracy: 0.1451\n",
      "Epoch 5/10\n",
      "565/565 [==============================] - 95s 169ms/step - loss: 3.9299 - accuracy: 0.1432 - categorical_accuracy: 0.1432 - val_loss: 3.9082 - val_accuracy: 0.1599 - val_categorical_accuracy: 0.1599\n",
      "Epoch 6/10\n",
      "565/565 [==============================] - 95s 169ms/step - loss: 3.8492 - accuracy: 0.1542 - categorical_accuracy: 0.1542 - val_loss: 3.8557 - val_accuracy: 0.1695 - val_categorical_accuracy: 0.1695\n",
      "Epoch 7/10\n",
      "565/565 [==============================] - 95s 169ms/step - loss: 3.7790 - accuracy: 0.1660 - categorical_accuracy: 0.1660 - val_loss: 3.7830 - val_accuracy: 0.1803 - val_categorical_accuracy: 0.1803\n",
      "Epoch 8/10\n",
      "565/565 [==============================] - 95s 169ms/step - loss: 3.7112 - accuracy: 0.1753 - categorical_accuracy: 0.1753 - val_loss: 3.7365 - val_accuracy: 0.1871 - val_categorical_accuracy: 0.1871\n",
      "Epoch 9/10\n",
      "565/565 [==============================] - 95s 168ms/step - loss: 3.6446 - accuracy: 0.1865 - categorical_accuracy: 0.1865 - val_loss: 3.7011 - val_accuracy: 0.1926 - val_categorical_accuracy: 0.1926\n",
      "Epoch 10/10\n",
      "565/565 [==============================] - 95s 169ms/step - loss: 3.5661 - accuracy: 0.1947 - categorical_accuracy: 0.1947 - val_loss: 3.6588 - val_accuracy: 0.2069 - val_categorical_accuracy: 0.2069\n",
      "Keras model saved as: model_log_hp_3_md_3.h5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: model_log_hp_3_md_3/assets\n",
      "SavedModel file saved under: model_log_hp_3_md_3\n",
      "History file saved as history_log_hp_3_md_3\n",
      "SavedModel file zipped as: model_log_hp_3_md_3\n",
      "File model_log_hp_3_md_3.h5 uploaded to Google Bucket: training_logs/data_split_1/model_log_hp_3_md_3.h5.\n",
      "File history_log_hp_3_md_3 uploaded to Google Bucket: training_logs/data_split_1/history_log_hp_3_md_3.\n",
      "File model_log_hp_3_md_3.zip uploaded to Google Bucket: training_logs/data_split_1/model_log_hp_3_md_3.zip.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 150, 150, 128)     3584      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 150, 150, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 75, 75, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 75, 75, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 37, 37, 128)       295040    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 175232)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               22429824  \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 102)               13158     \n",
      "=================================================================\n",
      "Total params: 23,774,438\n",
      "Trainable params: 23,774,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " \n",
      " ------------------------------------------------\n",
      "\n",
      " Training for hyperparameter 4 and model 3 .... \n",
      " \n",
      "\n",
      " train size: 18053, validation size: 9026, batch size: 35, epochs: 10 \n",
      "\n",
      "------------------------------------------------ \n",
      " \n",
      "\n",
      "Train for 516.0 steps, validate for 258.0 steps\n",
      "Epoch 1/10\n",
      "516/516 [==============================] - 171s 332ms/step - loss: 4.2165 - accuracy: 0.0981 - categorical_accuracy: 0.0981 - val_loss: 4.0037 - val_accuracy: 0.1328 - val_categorical_accuracy: 0.1328\n",
      "Epoch 2/10\n",
      "516/516 [==============================] - 107s 208ms/step - loss: 3.9629 - accuracy: 0.1316 - categorical_accuracy: 0.1316 - val_loss: 3.8836 - val_accuracy: 0.1518 - val_categorical_accuracy: 0.1518\n",
      "Epoch 3/10\n",
      "516/516 [==============================] - 107s 207ms/step - loss: 3.7785 - accuracy: 0.1549 - categorical_accuracy: 0.1549 - val_loss: 3.6831 - val_accuracy: 0.1754 - val_categorical_accuracy: 0.1754\n",
      "Epoch 4/10\n",
      "516/516 [==============================] - 107s 208ms/step - loss: 3.5044 - accuracy: 0.1969 - categorical_accuracy: 0.1969 - val_loss: 3.5614 - val_accuracy: 0.2080 - val_categorical_accuracy: 0.2080\n",
      "Epoch 5/10\n",
      "516/516 [==============================] - 107s 207ms/step - loss: 3.1039 - accuracy: 0.2689 - categorical_accuracy: 0.2689 - val_loss: 3.3869 - val_accuracy: 0.2526 - val_categorical_accuracy: 0.2526\n",
      "Epoch 6/10\n",
      "515/516 [============================>.] - ETA: 0s - loss: 2.6141 - accuracy: 0.3596 - categorical_accuracy: 0.3596\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "516/516 [==============================] - 107s 208ms/step - loss: 2.6135 - accuracy: 0.3596 - categorical_accuracy: 0.3596 - val_loss: 3.4078 - val_accuracy: 0.2791 - val_categorical_accuracy: 0.2791\n",
      "Epoch 7/10\n",
      "516/516 [==============================] - 107s 208ms/step - loss: 2.1335 - accuracy: 0.4591 - categorical_accuracy: 0.4591 - val_loss: 3.2458 - val_accuracy: 0.3359 - val_categorical_accuracy: 0.3359\n",
      "Epoch 8/10\n",
      "516/516 [==============================] - 107s 207ms/step - loss: 1.7778 - accuracy: 0.5358 - categorical_accuracy: 0.5358 - val_loss: 3.2100 - val_accuracy: 0.3693 - val_categorical_accuracy: 0.3693\n",
      "Epoch 9/10\n",
      "515/516 [============================>.] - ETA: 0s - loss: 1.4894 - accuracy: 0.6005 - categorical_accuracy: 0.6005\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.5999999595806004e-05.\n",
      "516/516 [==============================] - 107s 208ms/step - loss: 1.4886 - accuracy: 0.6008 - categorical_accuracy: 0.6008 - val_loss: 3.2453 - val_accuracy: 0.3900 - val_categorical_accuracy: 0.3900\n",
      "Epoch 10/10\n",
      "516/516 [==============================] - 107s 207ms/step - loss: 1.3368 - accuracy: 0.6333 - categorical_accuracy: 0.6333 - val_loss: 3.1654 - val_accuracy: 0.4334 - val_categorical_accuracy: 0.4334\n",
      "Keras model saved as: model_log_hp_4_md_3.h5\n",
      "INFO:tensorflow:Assets written to: model_log_hp_4_md_3/assets\n",
      "SavedModel file saved under: model_log_hp_4_md_3\n",
      "History file saved as history_log_hp_4_md_3\n",
      "SavedModel file zipped as: model_log_hp_4_md_3\n",
      "File model_log_hp_4_md_3.h5 uploaded to Google Bucket: training_logs/data_split_1/model_log_hp_4_md_3.h5.\n",
      "File history_log_hp_4_md_3 uploaded to Google Bucket: training_logs/data_split_1/history_log_hp_4_md_3.\n",
      "File model_log_hp_4_md_3.zip uploaded to Google Bucket: training_logs/data_split_1/model_log_hp_4_md_3.zip.\n"
     ]
    }
   ],
   "source": [
    "for i, hparam in enumerate(hypers):\n",
    "  \n",
    "  if i == 0 or i == 1 or i == 2:\n",
    "    continue\n",
    "\n",
    "  train_h = train_ds.cache() # cache the dataset in RAM or on disk\n",
    "  train_h = train_h.repeat() # repeat the dataset indefinitely\n",
    "  train_h = train_h.batch(hparam[\"batch\"]) # batch data elements together in batches of 128\n",
    "  train_h = train_h.prefetch(AUTOTUNE)\n",
    "\n",
    "  val_h = val_ds.cache() # cache the dataset in RAM or on disk\n",
    "  val_h = val_h.repeat() # repeat the dataset indefinitely\n",
    "  val_h = val_h.batch(hparam[\"batch\"]) # batch data elements together in batches of 128\n",
    "  val_h = val_h.prefetch(AUTOTUNE)\n",
    "\n",
    "  for j in range(0,5):\n",
    "    \n",
    "    if j == 0:\n",
    "      continue\n",
    "      model = create_model_1()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 1:\n",
    "      continue\n",
    "      model = create_model_2()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 2:\n",
    "      continue\n",
    "      model = create_model_3()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 3:\n",
    "      \n",
    "      model = create_model_4()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 4:\n",
    "      continue\n",
    "      model = create_model_5()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.4, verbose=1, patience=1, min_lr=0.0000001)\n",
    "\n",
    "    print(\"\\n \\n ------------------------------------------------\")\n",
    "    print(\"\\n Training for hyperparameter {} and model {} .... \\n \".format(i,j))\n",
    "    print(\"\\n train size: {}, validation size: {}, batch size: {}, epochs: 10 \\n\".format(train_size, val_size, hparam[\"batch\"]))\n",
    "    print(\"------------------------------------------------ \\n \\n\")\n",
    "\n",
    "    history = model.fit(train_h, epochs=hparam[\"epochs\"], validation_data=val_h, steps_per_epoch=hparam[\"steps\"], validation_steps=hparam[\"val_steps\"], callbacks=[reduce_lr])\n",
    "    \n",
    "    model.save(\"model_log_hp_{}_md_{}.h5\".format(i,j))\n",
    "    print(\"Keras model saved as: model_log_hp_{}_md_{}.h5\".format(i,j))\n",
    "    model.save('model_log_hp_{}_md_{}'.format(i,j), save_format='tf')\n",
    "    print(\"SavedModel file saved under: model_log_hp_{}_md_{}\".format(i,j))\n",
    "\n",
    "    with open('history_log_hp_{}_md_{}'.format(i,j), 'wb') as file_pi:\n",
    "      pickle.dump(history.history, file_pi)\n",
    "    print(\"History file saved as history_log_hp_{}_md_{}\".format(i,j))\n",
    "\n",
    "    zipf = zipfile.ZipFile('model_log_hp_{}_md_{}.zip'.format(i,j), 'w', zipfile.ZIP_DEFLATED)\n",
    "    zipdir('model_log_hp_{}_md_{}'.format(i,j), zipf)\n",
    "    zipf.close()\n",
    "    print(\"SavedModel file zipped as: model_log_hp_{}_md_{}\".format(i,j))\n",
    "    \n",
    "    upload_blob(\"model_log_hp_{}_md_{}.h5\".format(i,j), choice)\n",
    "    upload_blob(\"history_log_hp_{}_md_{}\".format(i,j), choice)\n",
    "    upload_blob(\"model_log_hp_{}_md_{}.zip\".format(i,j), choice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtfDgWX_b2KO"
   },
   "source": [
    "## Data Split: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0mELkanb5sb"
   },
   "outputs": [],
   "source": [
    "choice = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "_zkVyLvib-af",
    "outputId": "e09d4d17-177b-4f30-cd75-c4372d66029f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22566 13541 9026\n",
      "total:  45133\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds, train_size, test_size, val_size = data_split(full_ds, dataset_size, choice=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGIXjVdScBUo"
   },
   "outputs": [],
   "source": [
    "hypers = [{\"batch\": 25,\"steps\":np.ceil(train_size/25), \"val_steps\": np.ceil(val_size/25),\"optimizer\": tf.optimizers.SGD(learning_rate=0.01),\"epochs\": 10},\n",
    "                {\"batch\": 28,\"steps\":np.ceil(train_size/28), \"val_steps\": np.ceil(val_size/28), \"optimizer\": tf.optimizers.RMSprop(learning_rate=0.001),\"epochs\": 10},\n",
    "                {\"batch\": 30,\"steps\":np.ceil(train_size/30), \"val_steps\": np.ceil(val_size/30), \"optimizer\": tf.optimizers.Adagrad(learning_rate=0.0001),\"epochs\": 10},\n",
    "                {\"batch\": 32,\"steps\":np.ceil(train_size/32), \"val_steps\": np.ceil(val_size/32), \"optimizer\": tf.optimizers.Adam(learning_rate=0.00001),\"epochs\": 10},\n",
    "                {\"batch\": 35,\"steps\":np.ceil(train_size/35), \"val_steps\": np.ceil(val_size/35), \"optimizer\": tf.optimizers.Adam(learning_rate=0.0001),\"epochs\": 10}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5ulDxtiDcFFb",
    "outputId": "3f7a844c-89c3-42ee-c68b-173758da4005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 150, 150, 128)     3584      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 150, 150, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 75, 75, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 75, 75, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 37, 37, 128)       295040    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 175232)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               22429824  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 102)               13158     \n",
      "=================================================================\n",
      "Total params: 23,774,438\n",
      "Trainable params: 23,774,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " \n",
      " ------------------------------------------------\n",
      "\n",
      " Training for hyperparameter 2 and model 3 .... \n",
      " \n",
      "\n",
      " train size: 22566, validation size: 9026, batch size: 30, epochs: 10 \n",
      "\n",
      "------------------------------------------------ \n",
      " \n",
      "\n",
      "Train for 753.0 steps, validate for 301.0 steps\n",
      "Epoch 1/10\n",
      "753/753 [==============================] - 336s 447ms/step - loss: 4.4713 - accuracy: 0.0709 - categorical_accuracy: 0.0709 - val_loss: 4.3848 - val_accuracy: 0.0739 - val_categorical_accuracy: 0.0739\n",
      "Epoch 2/10\n",
      "753/753 [==============================] - 301s 400ms/step - loss: 4.3655 - accuracy: 0.0740 - categorical_accuracy: 0.0740 - val_loss: 4.3385 - val_accuracy: 0.0744 - val_categorical_accuracy: 0.0744\n",
      "Epoch 3/10\n",
      "753/753 [==============================] - 301s 399ms/step - loss: 4.3445 - accuracy: 0.0762 - categorical_accuracy: 0.0762 - val_loss: 4.3191 - val_accuracy: 0.0753 - val_categorical_accuracy: 0.0753\n",
      "Epoch 4/10\n",
      "753/753 [==============================] - 300s 399ms/step - loss: 4.3248 - accuracy: 0.0757 - categorical_accuracy: 0.0757 - val_loss: 4.3009 - val_accuracy: 0.0760 - val_categorical_accuracy: 0.0760\n",
      "Epoch 5/10\n",
      "753/753 [==============================] - 300s 399ms/step - loss: 4.3072 - accuracy: 0.0768 - categorical_accuracy: 0.0768 - val_loss: 4.2866 - val_accuracy: 0.0777 - val_categorical_accuracy: 0.0777\n",
      "Epoch 6/10\n",
      "753/753 [==============================] - 301s 399ms/step - loss: 4.2981 - accuracy: 0.0775 - categorical_accuracy: 0.0775 - val_loss: 4.2743 - val_accuracy: 0.0769 - val_categorical_accuracy: 0.0769\n",
      "Epoch 7/10\n",
      "753/753 [==============================] - 301s 399ms/step - loss: 4.2881 - accuracy: 0.0745 - categorical_accuracy: 0.0745 - val_loss: 4.2682 - val_accuracy: 0.0772 - val_categorical_accuracy: 0.0772\n",
      "Epoch 8/10\n",
      "753/753 [==============================] - 301s 400ms/step - loss: 4.2776 - accuracy: 0.0781 - categorical_accuracy: 0.0781 - val_loss: 4.2587 - val_accuracy: 0.0785 - val_categorical_accuracy: 0.0785\n",
      "Epoch 9/10\n",
      "753/753 [==============================] - 301s 400ms/step - loss: 4.2701 - accuracy: 0.0790 - categorical_accuracy: 0.0790 - val_loss: 4.2509 - val_accuracy: 0.0790 - val_categorical_accuracy: 0.0790\n",
      "Epoch 10/10\n",
      "753/753 [==============================] - 301s 399ms/step - loss: 4.2645 - accuracy: 0.0782 - categorical_accuracy: 0.0782 - val_loss: 4.2433 - val_accuracy: 0.0795 - val_categorical_accuracy: 0.0795\n",
      "Keras model saved as: model_log_hp_2_md_3.h5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: model_log_hp_2_md_3/assets\n",
      "SavedModel file saved under: model_log_hp_2_md_3\n",
      "History file saved as history_log_hp_2_md_3\n",
      "SavedModel file zipped as: model_log_hp_2_md_3\n",
      "File model_log_hp_2_md_3.h5 uploaded to Google Bucket: training_logs/data_split_2/model_log_hp_2_md_3.h5.\n",
      "File history_log_hp_2_md_3 uploaded to Google Bucket: training_logs/data_split_2/history_log_hp_2_md_3.\n",
      "File model_log_hp_2_md_3.zip uploaded to Google Bucket: training_logs/data_split_2/model_log_hp_2_md_3.zip.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 150, 150, 128)     3584      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 150, 150, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 75, 75, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 75, 75, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 37, 37, 128)       295040    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 175232)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               22429824  \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 102)               13158     \n",
      "=================================================================\n",
      "Total params: 23,774,438\n",
      "Trainable params: 23,774,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " \n",
      " ------------------------------------------------\n",
      "\n",
      " Training for hyperparameter 3 and model 3 .... \n",
      " \n",
      "\n",
      " train size: 22566, validation size: 9026, batch size: 32, epochs: 10 \n",
      "\n",
      "------------------------------------------------ \n",
      " \n",
      "\n",
      "Train for 706.0 steps, validate for 283.0 steps\n",
      "Epoch 1/10\n",
      "706/706 [==============================] - 316s 448ms/step - loss: 4.3587 - accuracy: 0.0773 - categorical_accuracy: 0.0773 - val_loss: 4.2725 - val_accuracy: 0.1028 - val_categorical_accuracy: 0.1028\n",
      "Epoch 2/10\n",
      "706/706 [==============================] - 295s 418ms/step - loss: 4.1868 - accuracy: 0.1134 - categorical_accuracy: 0.1134 - val_loss: 4.1276 - val_accuracy: 0.1317 - val_categorical_accuracy: 0.1317\n",
      "Epoch 3/10\n",
      "706/706 [==============================] - 295s 418ms/step - loss: 4.0716 - accuracy: 0.1267 - categorical_accuracy: 0.1267 - val_loss: 4.0378 - val_accuracy: 0.1388 - val_categorical_accuracy: 0.1388\n",
      "Epoch 4/10\n",
      "706/706 [==============================] - 296s 419ms/step - loss: 3.9840 - accuracy: 0.1375 - categorical_accuracy: 0.1375 - val_loss: 3.9553 - val_accuracy: 0.1443 - val_categorical_accuracy: 0.1443\n",
      "Epoch 5/10\n",
      "706/706 [==============================] - 295s 418ms/step - loss: 3.9091 - accuracy: 0.1458 - categorical_accuracy: 0.1458 - val_loss: 3.8944 - val_accuracy: 0.1567 - val_categorical_accuracy: 0.1567\n",
      "Epoch 6/10\n",
      "706/706 [==============================] - 295s 418ms/step - loss: 3.8428 - accuracy: 0.1551 - categorical_accuracy: 0.1551 - val_loss: 3.8498 - val_accuracy: 0.1678 - val_categorical_accuracy: 0.1678\n",
      "Epoch 7/10\n",
      "706/706 [==============================] - 295s 418ms/step - loss: 3.7782 - accuracy: 0.1695 - categorical_accuracy: 0.1695 - val_loss: 3.8074 - val_accuracy: 0.1757 - val_categorical_accuracy: 0.1757\n",
      "Epoch 8/10\n",
      "706/706 [==============================] - 295s 418ms/step - loss: 3.7117 - accuracy: 0.1747 - categorical_accuracy: 0.1747 - val_loss: 3.7524 - val_accuracy: 0.1887 - val_categorical_accuracy: 0.1887\n",
      "Epoch 9/10\n",
      "706/706 [==============================] - 295s 418ms/step - loss: 3.6550 - accuracy: 0.1851 - categorical_accuracy: 0.1851 - val_loss: 3.7105 - val_accuracy: 0.2013 - val_categorical_accuracy: 0.2013\n",
      "Epoch 10/10\n",
      "706/706 [==============================] - 295s 418ms/step - loss: 3.5841 - accuracy: 0.1966 - categorical_accuracy: 0.1966 - val_loss: 3.6489 - val_accuracy: 0.1989 - val_categorical_accuracy: 0.1989\n",
      "Keras model saved as: model_log_hp_3_md_3.h5\n",
      "INFO:tensorflow:Assets written to: model_log_hp_3_md_3/assets\n",
      "SavedModel file saved under: model_log_hp_3_md_3\n",
      "History file saved as history_log_hp_3_md_3\n",
      "SavedModel file zipped as: model_log_hp_3_md_3\n",
      "File model_log_hp_3_md_3.h5 uploaded to Google Bucket: training_logs/data_split_2/model_log_hp_3_md_3.h5.\n",
      "File history_log_hp_3_md_3 uploaded to Google Bucket: training_logs/data_split_2/history_log_hp_3_md_3.\n",
      "File model_log_hp_3_md_3.zip uploaded to Google Bucket: training_logs/data_split_2/model_log_hp_3_md_3.zip.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 150, 150, 128)     3584      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 150, 150, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 75, 75, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 75, 75, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 37, 37, 128)       295040    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 175232)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               22429824  \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 102)               13158     \n",
      "=================================================================\n",
      "Total params: 23,774,438\n",
      "Trainable params: 23,774,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " \n",
      " ------------------------------------------------\n",
      "\n",
      " Training for hyperparameter 4 and model 3 .... \n",
      " \n",
      "\n",
      " train size: 22566, validation size: 9026, batch size: 35, epochs: 10 \n",
      "\n",
      "------------------------------------------------ \n",
      " \n",
      "\n",
      "Train for 645.0 steps, validate for 258.0 steps\n",
      "Epoch 1/10\n",
      "645/645 [==============================] - 337s 522ms/step - loss: 4.2417 - accuracy: 0.0826 - categorical_accuracy: 0.0826 - val_loss: 4.0644 - val_accuracy: 0.1250 - val_categorical_accuracy: 0.1250\n",
      "Epoch 2/10\n",
      "645/645 [==============================] - 320s 496ms/step - loss: 4.0144 - accuracy: 0.1180 - categorical_accuracy: 0.1180 - val_loss: 3.9474 - val_accuracy: 0.1395 - val_categorical_accuracy: 0.1395\n",
      "Epoch 3/10\n",
      "645/645 [==============================] - 320s 495ms/step - loss: 3.8115 - accuracy: 0.1455 - categorical_accuracy: 0.1455 - val_loss: 3.7375 - val_accuracy: 0.1772 - val_categorical_accuracy: 0.1772\n",
      "Epoch 4/10\n",
      "645/645 [==============================] - 320s 496ms/step - loss: 3.5278 - accuracy: 0.1922 - categorical_accuracy: 0.1922 - val_loss: 3.4860 - val_accuracy: 0.2210 - val_categorical_accuracy: 0.2210\n",
      "Epoch 5/10\n",
      "645/645 [==============================] - 321s 497ms/step - loss: 3.1353 - accuracy: 0.2604 - categorical_accuracy: 0.2604 - val_loss: 3.2606 - val_accuracy: 0.2743 - val_categorical_accuracy: 0.2743\n",
      "Epoch 6/10\n",
      "645/645 [==============================] - 321s 497ms/step - loss: 2.6247 - accuracy: 0.3552 - categorical_accuracy: 0.3552 - val_loss: 3.1012 - val_accuracy: 0.3150 - val_categorical_accuracy: 0.3150\n",
      "Epoch 7/10\n",
      "645/645 [==============================] - 321s 498ms/step - loss: 2.0972 - accuracy: 0.4610 - categorical_accuracy: 0.4610 - val_loss: 2.9478 - val_accuracy: 0.3632 - val_categorical_accuracy: 0.3632\n",
      "Epoch 8/10\n",
      "645/645 [==============================] - 321s 498ms/step - loss: 1.6226 - accuracy: 0.5688 - categorical_accuracy: 0.5688 - val_loss: 2.7536 - val_accuracy: 0.4477 - val_categorical_accuracy: 0.4477\n",
      "Epoch 9/10\n",
      "644/645 [============================>.] - ETA: 0s - loss: 1.2576 - accuracy: 0.6520 - categorical_accuracy: 0.6520\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "645/645 [==============================] - 321s 498ms/step - loss: 1.2575 - accuracy: 0.6520 - categorical_accuracy: 0.6520 - val_loss: 2.8277 - val_accuracy: 0.4811 - val_categorical_accuracy: 0.4811\n",
      "Epoch 10/10\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.9949 - accuracy: 0.7146 - categorical_accuracy: 0.7146\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.5999999595806004e-05.\n",
      "645/645 [==============================] - 321s 498ms/step - loss: 0.9949 - accuracy: 0.7146 - categorical_accuracy: 0.7146 - val_loss: 2.7917 - val_accuracy: 0.5359 - val_categorical_accuracy: 0.5359\n",
      "Keras model saved as: model_log_hp_4_md_3.h5\n",
      "INFO:tensorflow:Assets written to: model_log_hp_4_md_3/assets\n",
      "SavedModel file saved under: model_log_hp_4_md_3\n",
      "History file saved as history_log_hp_4_md_3\n",
      "SavedModel file zipped as: model_log_hp_4_md_3\n",
      "File model_log_hp_4_md_3.h5 uploaded to Google Bucket: training_logs/data_split_2/model_log_hp_4_md_3.h5.\n",
      "File history_log_hp_4_md_3 uploaded to Google Bucket: training_logs/data_split_2/history_log_hp_4_md_3.\n",
      "File model_log_hp_4_md_3.zip uploaded to Google Bucket: training_logs/data_split_2/model_log_hp_4_md_3.zip.\n"
     ]
    }
   ],
   "source": [
    "for i, hparam in enumerate(hypers):\n",
    "\n",
    "  if i == 0 or i == 1:\n",
    "    continue\n",
    "\n",
    "  train_h = train_ds.cache() # cache the dataset in RAM or on disk\n",
    "  train_h = train_h.repeat() # repeat the dataset indefinitely\n",
    "  train_h = train_h.batch(hparam[\"batch\"]) # batch data elements together in batches of 128\n",
    "  train_h = train_h.prefetch(AUTOTUNE)\n",
    "\n",
    "  val_h = val_ds.cache() # cache the dataset in RAM or on disk\n",
    "  val_h = val_h.repeat() # repeat the dataset indefinitely\n",
    "  val_h = val_h.batch(hparam[\"batch\"]) # batch data elements together in batches of 128\n",
    "  val_h = val_h.prefetch(AUTOTUNE)\n",
    "\n",
    "  for j in range(0,5):\n",
    "    \n",
    "    if j == 0:\n",
    "      continue\n",
    "      model = create_model_1()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 1:\n",
    "      continue\n",
    "      model = create_model_2()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 2:\n",
    "      continue \n",
    "      model = create_model_3()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 3:\n",
    "      \n",
    "      model = create_model_4()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 4:\n",
    "      continue\n",
    "      model = create_model_5()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.4, verbose=1, patience=1, min_lr=0.0000001)\n",
    "\n",
    "    print(\"\\n \\n ------------------------------------------------\")\n",
    "    print(\"\\n Training for hyperparameter {} and model {} .... \\n \".format(i,j))\n",
    "    print(\"\\n train size: {}, validation size: {}, batch size: {}, epochs: 10 \\n\".format(train_size, val_size, hparam[\"batch\"]))\n",
    "    print(\"------------------------------------------------ \\n \\n\")\n",
    "\n",
    "    history = model.fit(train_h, epochs=hparam[\"epochs\"], validation_data=val_h, steps_per_epoch=hparam[\"steps\"], validation_steps=hparam[\"val_steps\"], callbacks=[reduce_lr])\n",
    "    \n",
    "    model.save(\"model_log_hp_{}_md_{}.h5\".format(i,j))\n",
    "    print(\"Keras model saved as: model_log_hp_{}_md_{}.h5\".format(i,j))\n",
    "    model.save('model_log_hp_{}_md_{}'.format(i,j), save_format='tf')\n",
    "    print(\"SavedModel file saved under: model_log_hp_{}_md_{}\".format(i,j))\n",
    "\n",
    "    with open('history_log_hp_{}_md_{}'.format(i,j), 'wb') as file_pi:\n",
    "      pickle.dump(history.history, file_pi)\n",
    "    print(\"History file saved as history_log_hp_{}_md_{}\".format(i,j))\n",
    "\n",
    "    zipf = zipfile.ZipFile('model_log_hp_{}_md_{}.zip'.format(i,j), 'w', zipfile.ZIP_DEFLATED)\n",
    "    zipdir('model_log_hp_{}_md_{}'.format(i,j), zipf)\n",
    "    zipf.close()\n",
    "    print(\"SavedModel file zipped as: model_log_hp_{}_md_{}\".format(i,j))\n",
    "    \n",
    "    upload_blob(\"model_log_hp_{}_md_{}.h5\".format(i,j), choice)\n",
    "    upload_blob(\"history_log_hp_{}_md_{}\".format(i,j), choice)\n",
    "    upload_blob(\"model_log_hp_{}_md_{}.zip\".format(i,j), choice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr7Sgk90kyfm"
   },
   "source": [
    "## Data Split: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWhI8ypZccis"
   },
   "outputs": [],
   "source": [
    "choice = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "-8mpH5Xtk470",
    "outputId": "a8e75596-e36a-4417-8c5d-eef7cbb8c838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27079 13541 4513\n",
      "total:  45133\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds, train_size, test_size, val_size = data_split(full_ds, dataset_size, choice=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ty0je8KCk72v"
   },
   "outputs": [],
   "source": [
    "hypers = [{\"batch\": 25,\"steps\":np.ceil(train_size/25), \"val_steps\": np.ceil(val_size/25),\"optimizer\": tf.optimizers.SGD(learning_rate=0.01),\"epochs\": 10},\n",
    "                {\"batch\": 28,\"steps\":np.ceil(train_size/28), \"val_steps\": np.ceil(val_size/28), \"optimizer\": tf.optimizers.RMSprop(learning_rate=0.001),\"epochs\": 10},\n",
    "                {\"batch\": 30,\"steps\":np.ceil(train_size/30), \"val_steps\": np.ceil(val_size/30), \"optimizer\": tf.optimizers.Adagrad(learning_rate=0.0001),\"epochs\": 10},\n",
    "                {\"batch\": 32,\"steps\":np.ceil(train_size/32), \"val_steps\": np.ceil(val_size/32), \"optimizer\": tf.optimizers.Adam(learning_rate=0.00001),\"epochs\": 10},\n",
    "                {\"batch\": 35,\"steps\":np.ceil(train_size/35), \"val_steps\": np.ceil(val_size/35), \"optimizer\": tf.optimizers.Adam(learning_rate=0.0001),\"epochs\": 10}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lNmPq79clFYI",
    "outputId": "a0f2b233-12f1-46a6-8427-361e59c83b9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 150, 150, 128)     3584      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 150, 150, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 75, 75, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 75, 75, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 37, 37, 128)       295040    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 175232)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               22429824  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 102)               13158     \n",
      "=================================================================\n",
      "Total params: 23,774,438\n",
      "Trainable params: 23,774,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " \n",
      " ------------------------------------------------\n",
      "\n",
      " Training for hyperparameter 0 and model 3 .... \n",
      " \n",
      "\n",
      " train size: 27079, validation size: 4513, batch size: 25, epochs: 10 \n",
      "\n",
      "------------------------------------------------ \n",
      " \n",
      "\n",
      "Train for 1084.0 steps, validate for 181.0 steps\n",
      "Epoch 1/10\n",
      "1084/1084 [==============================] - 252s 232ms/step - loss: 4.2077 - accuracy: 0.0767 - categorical_accuracy: 0.0767 - val_loss: 4.1323 - val_accuracy: 0.0997 - val_categorical_accuracy: 0.0997\n",
      "Epoch 2/10\n",
      "1084/1084 [==============================] - 142s 131ms/step - loss: 4.0963 - accuracy: 0.0927 - categorical_accuracy: 0.0927 - val_loss: 4.0592 - val_accuracy: 0.1169 - val_categorical_accuracy: 0.1169\n",
      "Epoch 3/10\n",
      "1084/1084 [==============================] - 142s 131ms/step - loss: 3.9552 - accuracy: 0.1201 - categorical_accuracy: 0.1201 - val_loss: 3.8932 - val_accuracy: 0.1534 - val_categorical_accuracy: 0.1534\n",
      "Epoch 4/10\n",
      "1084/1084 [==============================] - 142s 131ms/step - loss: 3.8043 - accuracy: 0.1449 - categorical_accuracy: 0.1449 - val_loss: 3.7135 - val_accuracy: 0.1662 - val_categorical_accuracy: 0.1662\n",
      "Epoch 5/10\n",
      "1084/1084 [==============================] - 142s 131ms/step - loss: 3.6673 - accuracy: 0.1653 - categorical_accuracy: 0.1653 - val_loss: 3.5316 - val_accuracy: 0.2002 - val_categorical_accuracy: 0.2002\n",
      "Epoch 6/10\n",
      "1084/1084 [==============================] - 141s 130ms/step - loss: 3.5426 - accuracy: 0.1883 - categorical_accuracy: 0.1883 - val_loss: 3.4175 - val_accuracy: 0.2241 - val_categorical_accuracy: 0.2241\n",
      "Epoch 7/10\n",
      "1084/1084 [==============================] - 141s 130ms/step - loss: 3.4128 - accuracy: 0.2103 - categorical_accuracy: 0.2103 - val_loss: 3.3302 - val_accuracy: 0.2351 - val_categorical_accuracy: 0.2351\n",
      "Epoch 8/10\n",
      "1084/1084 [==============================] - 141s 130ms/step - loss: 3.2584 - accuracy: 0.2377 - categorical_accuracy: 0.2377 - val_loss: 3.1383 - val_accuracy: 0.2767 - val_categorical_accuracy: 0.2767\n",
      "Epoch 9/10\n",
      "1084/1084 [==============================] - 141s 130ms/step - loss: 3.0548 - accuracy: 0.2748 - categorical_accuracy: 0.2748 - val_loss: 2.9370 - val_accuracy: 0.3257 - val_categorical_accuracy: 0.3257\n",
      "Epoch 10/10\n",
      "1084/1084 [==============================] - 142s 131ms/step - loss: 2.7820 - accuracy: 0.3262 - categorical_accuracy: 0.3262 - val_loss: 2.7189 - val_accuracy: 0.3801 - val_categorical_accuracy: 0.3801\n",
      "Keras model saved as: model_log_hp_0_md_3.h5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: model_log_hp_0_md_3/assets\n",
      "SavedModel file saved under: model_log_hp_0_md_3\n",
      "History file saved as history_log_hp_0_md_3\n",
      "SavedModel file zipped as: model_log_hp_0_md_3\n",
      "File model_log_hp_0_md_3.h5 uploaded to Google Bucket: training_logs/data_split_3/model_log_hp_0_md_3.h5.\n",
      "File history_log_hp_0_md_3 uploaded to Google Bucket: training_logs/data_split_3/history_log_hp_0_md_3.\n",
      "File model_log_hp_0_md_3.zip uploaded to Google Bucket: training_logs/data_split_3/model_log_hp_0_md_3.zip.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 150, 150, 128)     3584      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 150, 150, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 75, 75, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 75, 75, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 37, 37, 128)       295040    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 175232)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               22429824  \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 102)               13158     \n",
      "=================================================================\n",
      "Total params: 23,774,438\n",
      "Trainable params: 23,774,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " \n",
      " ------------------------------------------------\n",
      "\n",
      " Training for hyperparameter 1 and model 3 .... \n",
      " \n",
      "\n",
      " train size: 27079, validation size: 4513, batch size: 28, epochs: 10 \n",
      "\n",
      "------------------------------------------------ \n",
      " \n",
      "\n",
      "Train for 968.0 steps, validate for 162.0 steps\n",
      "Epoch 1/10\n",
      "968/968 [==============================] - 216s 223ms/step - loss: 4.3739 - accuracy: 0.0704 - categorical_accuracy: 0.0704 - val_loss: 4.1206 - val_accuracy: 0.0754 - val_categorical_accuracy: 0.0754\n",
      "Epoch 2/10\n",
      "968/968 [==============================] - 139s 143ms/step - loss: 4.1286 - accuracy: 0.0990 - categorical_accuracy: 0.0990 - val_loss: 3.9830 - val_accuracy: 0.1241 - val_categorical_accuracy: 0.1241\n",
      "Epoch 3/10\n",
      "968/968 [==============================] - 139s 144ms/step - loss: 3.9414 - accuracy: 0.1248 - categorical_accuracy: 0.1248 - val_loss: 3.7859 - val_accuracy: 0.1424 - val_categorical_accuracy: 0.1424\n",
      "Epoch 4/10\n",
      "968/968 [==============================] - 139s 143ms/step - loss: 3.8124 - accuracy: 0.1410 - categorical_accuracy: 0.1410 - val_loss: 3.6417 - val_accuracy: 0.1631 - val_categorical_accuracy: 0.1631\n",
      "Epoch 5/10\n",
      "968/968 [==============================] - 139s 143ms/step - loss: 3.7087 - accuracy: 0.1572 - categorical_accuracy: 0.1572 - val_loss: 3.6150 - val_accuracy: 0.1678 - val_categorical_accuracy: 0.1678\n",
      "Epoch 6/10\n",
      "967/968 [============================>.] - ETA: 0s - loss: 3.6349 - accuracy: 0.1714 - categorical_accuracy: 0.1714\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "968/968 [==============================] - 140s 144ms/step - loss: 3.6349 - accuracy: 0.1714 - categorical_accuracy: 0.1714 - val_loss: 3.6201 - val_accuracy: 0.1684 - val_categorical_accuracy: 0.1684\n",
      "Epoch 7/10\n",
      "968/968 [==============================] - 140s 144ms/step - loss: 3.4028 - accuracy: 0.2031 - categorical_accuracy: 0.2031 - val_loss: 3.4368 - val_accuracy: 0.2013 - val_categorical_accuracy: 0.2013\n",
      "Epoch 8/10\n",
      "968/968 [==============================] - 140s 144ms/step - loss: 3.3251 - accuracy: 0.2143 - categorical_accuracy: 0.2143 - val_loss: 3.3716 - val_accuracy: 0.2172 - val_categorical_accuracy: 0.2172\n",
      "Epoch 9/10\n",
      "968/968 [==============================] - 139s 144ms/step - loss: 3.2939 - accuracy: 0.2270 - categorical_accuracy: 0.2270 - val_loss: 3.3638 - val_accuracy: 0.2304 - val_categorical_accuracy: 0.2304\n",
      "Epoch 10/10\n",
      "967/968 [============================>.] - ETA: 0s - loss: 3.2958 - accuracy: 0.2316 - categorical_accuracy: 0.2316\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.\n",
      "968/968 [==============================] - 140s 144ms/step - loss: 3.2959 - accuracy: 0.2316 - categorical_accuracy: 0.2316 - val_loss: 3.4802 - val_accuracy: 0.2209 - val_categorical_accuracy: 0.2209\n",
      "Keras model saved as: model_log_hp_1_md_3.h5\n",
      "INFO:tensorflow:Assets written to: model_log_hp_1_md_3/assets\n",
      "SavedModel file saved under: model_log_hp_1_md_3\n",
      "History file saved as history_log_hp_1_md_3\n",
      "SavedModel file zipped as: model_log_hp_1_md_3\n",
      "File model_log_hp_1_md_3.h5 uploaded to Google Bucket: training_logs/data_split_3/model_log_hp_1_md_3.h5.\n",
      "File history_log_hp_1_md_3 uploaded to Google Bucket: training_logs/data_split_3/history_log_hp_1_md_3.\n",
      "File model_log_hp_1_md_3.zip uploaded to Google Bucket: training_logs/data_split_3/model_log_hp_1_md_3.zip.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 150, 150, 128)     3584      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 150, 150, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 75, 75, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 75, 75, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 37, 37, 128)       295040    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 175232)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               22429824  \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 102)               13158     \n",
      "=================================================================\n",
      "Total params: 23,774,438\n",
      "Trainable params: 23,774,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " \n",
      " ------------------------------------------------\n",
      "\n",
      " Training for hyperparameter 2 and model 3 .... \n",
      " \n",
      "\n",
      " train size: 27079, validation size: 4513, batch size: 30, epochs: 10 \n",
      "\n",
      "------------------------------------------------ \n",
      " \n",
      "\n",
      "Train for 903.0 steps, validate for 151.0 steps\n",
      "Epoch 1/10\n",
      "903/903 [==============================] - 226s 250ms/step - loss: 4.4263 - accuracy: 0.0722 - categorical_accuracy: 0.0722 - val_loss: 4.3568 - val_accuracy: 0.0682 - val_categorical_accuracy: 0.0682\n",
      "Epoch 2/10\n",
      "903/903 [==============================] - 135s 150ms/step - loss: 4.3394 - accuracy: 0.0716 - categorical_accuracy: 0.0716 - val_loss: 4.3292 - val_accuracy: 0.0671 - val_categorical_accuracy: 0.0671\n",
      "Epoch 3/10\n",
      "903/903 [==============================] - 135s 150ms/step - loss: 4.3246 - accuracy: 0.0747 - categorical_accuracy: 0.0747 - val_loss: 4.3167 - val_accuracy: 0.0706 - val_categorical_accuracy: 0.0706\n",
      "Epoch 4/10\n",
      "903/903 [==============================] - 135s 150ms/step - loss: 4.3158 - accuracy: 0.0751 - categorical_accuracy: 0.0751 - val_loss: 4.3075 - val_accuracy: 0.0706 - val_categorical_accuracy: 0.0706\n",
      "Epoch 5/10\n",
      "903/903 [==============================] - 135s 150ms/step - loss: 4.2974 - accuracy: 0.0742 - categorical_accuracy: 0.0742 - val_loss: 4.2918 - val_accuracy: 0.0728 - val_categorical_accuracy: 0.0728\n",
      "Epoch 6/10\n",
      "903/903 [==============================] - 135s 149ms/step - loss: 4.2902 - accuracy: 0.0767 - categorical_accuracy: 0.0767 - val_loss: 4.2836 - val_accuracy: 0.0784 - val_categorical_accuracy: 0.0784\n",
      "Epoch 7/10\n",
      "903/903 [==============================] - 135s 150ms/step - loss: 4.2776 - accuracy: 0.0767 - categorical_accuracy: 0.0767 - val_loss: 4.2729 - val_accuracy: 0.0770 - val_categorical_accuracy: 0.0770\n",
      "Epoch 8/10\n",
      "903/903 [==============================] - 134s 149ms/step - loss: 4.2672 - accuracy: 0.0788 - categorical_accuracy: 0.0788 - val_loss: 4.2668 - val_accuracy: 0.0775 - val_categorical_accuracy: 0.0775\n",
      "Epoch 9/10\n",
      "903/903 [==============================] - 133s 148ms/step - loss: 4.2622 - accuracy: 0.0776 - categorical_accuracy: 0.0776 - val_loss: 4.2583 - val_accuracy: 0.0804 - val_categorical_accuracy: 0.0804\n",
      "Epoch 10/10\n",
      "903/903 [==============================] - 134s 148ms/step - loss: 4.2485 - accuracy: 0.0807 - categorical_accuracy: 0.0807 - val_loss: 4.2506 - val_accuracy: 0.0821 - val_categorical_accuracy: 0.0821\n",
      "Keras model saved as: model_log_hp_2_md_3.h5\n",
      "INFO:tensorflow:Assets written to: model_log_hp_2_md_3/assets\n",
      "SavedModel file saved under: model_log_hp_2_md_3\n",
      "History file saved as history_log_hp_2_md_3\n",
      "SavedModel file zipped as: model_log_hp_2_md_3\n",
      "File model_log_hp_2_md_3.h5 uploaded to Google Bucket: training_logs/data_split_3/model_log_hp_2_md_3.h5.\n",
      "File history_log_hp_2_md_3 uploaded to Google Bucket: training_logs/data_split_3/history_log_hp_2_md_3.\n",
      "File model_log_hp_2_md_3.zip uploaded to Google Bucket: training_logs/data_split_3/model_log_hp_2_md_3.zip.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 150, 150, 128)     3584      \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 150, 150, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 75, 75, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 75, 75, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 37, 37, 128)       295040    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 175232)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               22429824  \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 102)               13158     \n",
      "=================================================================\n",
      "Total params: 23,774,438\n",
      "Trainable params: 23,774,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " \n",
      " ------------------------------------------------\n",
      "\n",
      " Training for hyperparameter 3 and model 3 .... \n",
      " \n",
      "\n",
      " train size: 27079, validation size: 4513, batch size: 32, epochs: 10 \n",
      "\n",
      "------------------------------------------------ \n",
      " \n",
      "\n",
      "Train for 847.0 steps, validate for 142.0 steps\n",
      "Epoch 1/10\n",
      "847/847 [==============================] - 223s 264ms/step - loss: 4.3530 - accuracy: 0.0763 - categorical_accuracy: 0.0763 - val_loss: 4.1859 - val_accuracy: 0.1254 - val_categorical_accuracy: 0.1254\n",
      "Epoch 2/10\n",
      "847/847 [==============================] - 130s 153ms/step - loss: 4.1557 - accuracy: 0.1143 - categorical_accuracy: 0.1143 - val_loss: 4.0289 - val_accuracy: 0.1367 - val_categorical_accuracy: 0.1367\n",
      "Epoch 3/10\n",
      "847/847 [==============================] - 130s 154ms/step - loss: 4.0336 - accuracy: 0.1274 - categorical_accuracy: 0.1274 - val_loss: 3.9173 - val_accuracy: 0.1501 - val_categorical_accuracy: 0.1501\n",
      "Epoch 4/10\n",
      "847/847 [==============================] - 130s 154ms/step - loss: 3.9396 - accuracy: 0.1412 - categorical_accuracy: 0.1412 - val_loss: 3.8440 - val_accuracy: 0.1604 - val_categorical_accuracy: 0.1604\n",
      "Epoch 5/10\n",
      "847/847 [==============================] - 131s 154ms/step - loss: 3.8544 - accuracy: 0.1543 - categorical_accuracy: 0.1543 - val_loss: 3.7643 - val_accuracy: 0.1732 - val_categorical_accuracy: 0.1732\n",
      "Epoch 6/10\n",
      "847/847 [==============================] - 130s 153ms/step - loss: 3.7806 - accuracy: 0.1679 - categorical_accuracy: 0.1679 - val_loss: 3.6987 - val_accuracy: 0.1849 - val_categorical_accuracy: 0.1849\n",
      "Epoch 7/10\n",
      "847/847 [==============================] - 129s 153ms/step - loss: 3.7010 - accuracy: 0.1753 - categorical_accuracy: 0.1753 - val_loss: 3.6445 - val_accuracy: 0.1948 - val_categorical_accuracy: 0.1948\n",
      "Epoch 8/10\n",
      "847/847 [==============================] - 130s 153ms/step - loss: 3.6346 - accuracy: 0.1881 - categorical_accuracy: 0.1881 - val_loss: 3.5606 - val_accuracy: 0.2102 - val_categorical_accuracy: 0.2102\n",
      "Epoch 9/10\n",
      "847/847 [==============================] - 130s 153ms/step - loss: 3.5566 - accuracy: 0.1982 - categorical_accuracy: 0.1982 - val_loss: 3.5050 - val_accuracy: 0.2194 - val_categorical_accuracy: 0.2194\n",
      "Epoch 10/10\n",
      "847/847 [==============================] - 130s 153ms/step - loss: 3.4781 - accuracy: 0.2124 - categorical_accuracy: 0.2124 - val_loss: 3.4413 - val_accuracy: 0.2300 - val_categorical_accuracy: 0.2300\n",
      "Keras model saved as: model_log_hp_3_md_3.h5\n",
      "INFO:tensorflow:Assets written to: model_log_hp_3_md_3/assets\n",
      "SavedModel file saved under: model_log_hp_3_md_3\n",
      "History file saved as history_log_hp_3_md_3\n",
      "SavedModel file zipped as: model_log_hp_3_md_3\n",
      "File model_log_hp_3_md_3.h5 uploaded to Google Bucket: training_logs/data_split_3/model_log_hp_3_md_3.h5.\n",
      "File history_log_hp_3_md_3 uploaded to Google Bucket: training_logs/data_split_3/history_log_hp_3_md_3.\n",
      "File model_log_hp_3_md_3.zip uploaded to Google Bucket: training_logs/data_split_3/model_log_hp_3_md_3.zip.\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 150, 150, 128)     3584      \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 150, 150, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 75, 75, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 75, 75, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 37, 37, 128)       295040    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 175232)            0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               22429824  \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 102)               13158     \n",
      "=================================================================\n",
      "Total params: 23,774,438\n",
      "Trainable params: 23,774,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " \n",
      " ------------------------------------------------\n",
      "\n",
      " Training for hyperparameter 4 and model 3 .... \n",
      " \n",
      "\n",
      " train size: 27079, validation size: 4513, batch size: 35, epochs: 10 \n",
      "\n",
      "------------------------------------------------ \n",
      " \n",
      "\n",
      "Train for 774.0 steps, validate for 129.0 steps\n",
      "Epoch 1/10\n",
      "774/774 [==============================] - 232s 300ms/step - loss: 4.2066 - accuracy: 0.0966 - categorical_accuracy: 0.0966 - val_loss: 4.0349 - val_accuracy: 0.1262 - val_categorical_accuracy: 0.1262\n",
      "Epoch 2/10\n",
      "774/774 [==============================] - 146s 188ms/step - loss: 3.9819 - accuracy: 0.1276 - categorical_accuracy: 0.1276 - val_loss: 3.8570 - val_accuracy: 0.1491 - val_categorical_accuracy: 0.1491\n",
      "Epoch 3/10\n",
      "774/774 [==============================] - 146s 188ms/step - loss: 3.8053 - accuracy: 0.1560 - categorical_accuracy: 0.1560 - val_loss: 3.6951 - val_accuracy: 0.1854 - val_categorical_accuracy: 0.1854\n",
      "Epoch 4/10\n",
      "774/774 [==============================] - 145s 188ms/step - loss: 3.6134 - accuracy: 0.1832 - categorical_accuracy: 0.1832 - val_loss: 3.5278 - val_accuracy: 0.2190 - val_categorical_accuracy: 0.2190\n",
      "Epoch 5/10\n",
      "774/774 [==============================] - 145s 187ms/step - loss: 3.3687 - accuracy: 0.2210 - categorical_accuracy: 0.2210 - val_loss: 3.3707 - val_accuracy: 0.2509 - val_categorical_accuracy: 0.2509\n",
      "Epoch 6/10\n",
      "774/774 [==============================] - 146s 188ms/step - loss: 3.0505 - accuracy: 0.2746 - categorical_accuracy: 0.2746 - val_loss: 3.1934 - val_accuracy: 0.2824 - val_categorical_accuracy: 0.2824\n",
      "Epoch 7/10\n",
      "774/774 [==============================] - 146s 188ms/step - loss: 2.6782 - accuracy: 0.3403 - categorical_accuracy: 0.3403 - val_loss: 3.0400 - val_accuracy: 0.3280 - val_categorical_accuracy: 0.3280\n",
      "Epoch 8/10\n",
      "774/774 [==============================] - 145s 187ms/step - loss: 2.3222 - accuracy: 0.4093 - categorical_accuracy: 0.4093 - val_loss: 2.8723 - val_accuracy: 0.3717 - val_categorical_accuracy: 0.3717\n",
      "Epoch 9/10\n",
      "774/774 [==============================] - 145s 187ms/step - loss: 2.0124 - accuracy: 0.4729 - categorical_accuracy: 0.4729 - val_loss: 2.7018 - val_accuracy: 0.4308 - val_categorical_accuracy: 0.4308\n",
      "Epoch 10/10\n",
      "774/774 [==============================] - 145s 187ms/step - loss: 1.7212 - accuracy: 0.5307 - categorical_accuracy: 0.5307 - val_loss: 2.6708 - val_accuracy: 0.4651 - val_categorical_accuracy: 0.4651\n",
      "Keras model saved as: model_log_hp_4_md_3.h5\n",
      "INFO:tensorflow:Assets written to: model_log_hp_4_md_3/assets\n",
      "SavedModel file saved under: model_log_hp_4_md_3\n",
      "History file saved as history_log_hp_4_md_3\n",
      "SavedModel file zipped as: model_log_hp_4_md_3\n",
      "File model_log_hp_4_md_3.h5 uploaded to Google Bucket: training_logs/data_split_3/model_log_hp_4_md_3.h5.\n",
      "File history_log_hp_4_md_3 uploaded to Google Bucket: training_logs/data_split_3/history_log_hp_4_md_3.\n",
      "File model_log_hp_4_md_3.zip uploaded to Google Bucket: training_logs/data_split_3/model_log_hp_4_md_3.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenet_1.00_224 (Model)   (None, 4, 4, 1024)        3228864   \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               4194560   \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 102)               26214     \n",
      "=================================================================\n",
      "Total params: 7,449,638\n",
      "Trainable params: 7,427,750\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n",
      "\n",
      " \n",
      " ------------------------------------------------\n",
      "\n",
      " Training for hyperparameter 4 and model 4 .... \n",
      " \n",
      "\n",
      " train size: 27079, validation size: 4513, batch size: 35, epochs: 10 \n",
      "\n",
      "------------------------------------------------ \n",
      " \n",
      "\n",
      "Train for 774.0 steps, validate for 129.0 steps\n",
      "Epoch 1/10\n",
      "774/774 [==============================] - 70s 90ms/step - loss: 3.7508 - accuracy: 0.2145 - categorical_accuracy: 0.2145 - val_loss: 2.9843 - val_accuracy: 0.3320 - val_categorical_accuracy: 0.3320\n",
      "Epoch 2/10\n",
      "774/774 [==============================] - 65s 84ms/step - loss: 2.8940 - accuracy: 0.3362 - categorical_accuracy: 0.3362 - val_loss: 2.3763 - val_accuracy: 0.4416 - val_categorical_accuracy: 0.4416\n",
      "Epoch 3/10\n",
      "774/774 [==============================] - 65s 84ms/step - loss: 2.3149 - accuracy: 0.4388 - categorical_accuracy: 0.4388 - val_loss: 2.0034 - val_accuracy: 0.5240 - val_categorical_accuracy: 0.5240\n",
      "Epoch 4/10\n",
      "774/774 [==============================] - 65s 84ms/step - loss: 1.8763 - accuracy: 0.5200 - categorical_accuracy: 0.5200 - val_loss: 1.7566 - val_accuracy: 0.5803 - val_categorical_accuracy: 0.5803\n",
      "Epoch 5/10\n",
      "774/774 [==============================] - 65s 84ms/step - loss: 1.4890 - accuracy: 0.6052 - categorical_accuracy: 0.6052 - val_loss: 1.5610 - val_accuracy: 0.6312 - val_categorical_accuracy: 0.6312\n",
      "Epoch 6/10\n",
      "774/774 [==============================] - 66s 85ms/step - loss: 1.2116 - accuracy: 0.6700 - categorical_accuracy: 0.6700 - val_loss: 1.3690 - val_accuracy: 0.6864 - val_categorical_accuracy: 0.6864\n",
      "Epoch 7/10\n",
      "774/774 [==============================] - 66s 85ms/step - loss: 0.9776 - accuracy: 0.7274 - categorical_accuracy: 0.7274 - val_loss: 1.2975 - val_accuracy: 0.7134 - val_categorical_accuracy: 0.7134\n",
      "Epoch 8/10\n",
      "774/774 [==============================] - 65s 84ms/step - loss: 0.8031 - accuracy: 0.7752 - categorical_accuracy: 0.7752 - val_loss: 1.2375 - val_accuracy: 0.7406 - val_categorical_accuracy: 0.7406\n",
      "Epoch 9/10\n",
      "774/774 [==============================] - 65s 84ms/step - loss: 0.6593 - accuracy: 0.8111 - categorical_accuracy: 0.8111 - val_loss: 1.2327 - val_accuracy: 0.7473 - val_categorical_accuracy: 0.7473\n",
      "Epoch 10/10\n",
      "773/774 [============================>.] - ETA: 0s - loss: 0.5477 - accuracy: 0.8460 - categorical_accuracy: 0.8460\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "774/774 [==============================] - 65s 84ms/step - loss: 0.5475 - accuracy: 0.8461 - categorical_accuracy: 0.8461 - val_loss: 1.2337 - val_accuracy: 0.7692 - val_categorical_accuracy: 0.7692\n",
      "Keras model saved as: model_log_hp_4_md_4.h5\n",
      "INFO:tensorflow:Assets written to: model_log_hp_4_md_4/assets\n",
      "SavedModel file saved under: model_log_hp_4_md_4\n",
      "History file saved as history_log_hp_4_md_4\n",
      "SavedModel file zipped as: model_log_hp_4_md_4\n",
      "File model_log_hp_4_md_4.h5 uploaded to Google Bucket: training_logs/data_split_3/model_log_hp_4_md_4.h5.\n",
      "File history_log_hp_4_md_4 uploaded to Google Bucket: training_logs/data_split_3/history_log_hp_4_md_4.\n",
      "File model_log_hp_4_md_4.zip uploaded to Google Bucket: training_logs/data_split_3/model_log_hp_4_md_4.zip.\n"
     ]
    }
   ],
   "source": [
    "for i, hparam in enumerate(hypers):\n",
    "\n",
    "  train_h = train_ds.cache() # cache the dataset in RAM or on disk\n",
    "  train_h = train_h.repeat() # repeat the dataset indefinitely\n",
    "  train_h = train_h.batch(hparam[\"batch\"]) # batch data elements together in batches of 128\n",
    "  train_h = train_h.prefetch(AUTOTUNE)\n",
    "\n",
    "  val_h = val_ds.cache() # cache the dataset in RAM or on disk\n",
    "  val_h = val_h.repeat() # repeat the dataset indefinitely\n",
    "  val_h = val_h.batch(hparam[\"batch\"]) # batch data elements together in batches of 128\n",
    "  val_h = val_h.prefetch(AUTOTUNE)\n",
    "\n",
    "  for j in range(0,5):\n",
    "    \n",
    "    if j == 0:\n",
    "      model = create_model_1()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 1:\n",
    "      model = create_model_2()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 2:\n",
    "      model = create_model_3()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 3:\n",
    "      continue\n",
    "      model = create_model_4()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 4:\n",
    "      continue\n",
    "      model = create_model_5()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.4, verbose=1, patience=1, min_lr=0.0000001)\n",
    "\n",
    "    print(\"\\n \\n ------------------------------------------------\")\n",
    "    print(\"\\n Training for hyperparameter {} and model {} .... \\n \".format(i,j))\n",
    "    print(\"\\n train size: {}, validation size: {}, batch size: {}, epochs: 10 \\n\".format(train_size, val_size, hparam[\"batch\"]))\n",
    "    print(\"------------------------------------------------ \\n \\n\")\n",
    "\n",
    "    history = model.fit(train_h, epochs=hparam[\"epochs\"], validation_data=val_h, steps_per_epoch=hparam[\"steps\"], validation_steps=hparam[\"val_steps\"], callbacks=[reduce_lr])\n",
    "    \n",
    "    model.save(\"model_log_hp_{}_md_{}.h5\".format(i,j))\n",
    "    print(\"Keras model saved as: model_log_hp_{}_md_{}.h5\".format(i,j))\n",
    "    model.save('model_log_hp_{}_md_{}'.format(i,j), save_format='tf')\n",
    "    print(\"SavedModel file saved under: model_log_hp_{}_md_{}\".format(i,j))\n",
    "\n",
    "    with open('history_log_hp_{}_md_{}'.format(i,j), 'wb') as file_pi:\n",
    "      pickle.dump(history.history, file_pi)\n",
    "    print(\"History file saved as history_log_hp_{}_md_{}\".format(i,j))\n",
    "\n",
    "    zipf = zipfile.ZipFile('model_log_hp_{}_md_{}.zip'.format(i,j), 'w', zipfile.ZIP_DEFLATED)\n",
    "    zipdir('model_log_hp_{}_md_{}'.format(i,j), zipf)\n",
    "    zipf.close()\n",
    "    print(\"SavedModel file zipped as: model_log_hp_{}_md_{}\".format(i,j))\n",
    "    \n",
    "    upload_blob(\"model_log_hp_{}_md_{}.h5\".format(i,j), choice)\n",
    "    upload_blob(\"history_log_hp_{}_md_{}\".format(i,j), choice)\n",
    "    upload_blob(\"model_log_hp_{}_md_{}.zip\".format(i,j), choice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e-IJMNFdUsA"
   },
   "source": [
    "## Data Split : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7vLrfzClUch"
   },
   "outputs": [],
   "source": [
    "choice = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "Saz2bCSIdZxh",
    "outputId": "f3a6dc7e-5684-4fe2-b3a3-86817ce9eb94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29336 9028 6769\n",
      "total:  45133\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds, train_size, test_size, val_size = data_split(full_ds, dataset_size, choice=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J11HGaLCdckJ"
   },
   "outputs": [],
   "source": [
    "hypers = [{\"batch\": 25,\"steps\":np.ceil(train_size/25), \"val_steps\": np.ceil(val_size/25),\"optimizer\": tf.optimizers.SGD(learning_rate=0.01),\"epochs\": 10},\n",
    "                {\"batch\": 28,\"steps\":np.ceil(train_size/28), \"val_steps\": np.ceil(val_size/28), \"optimizer\": tf.optimizers.RMSprop(learning_rate=0.001),\"epochs\": 10},\n",
    "                {\"batch\": 30,\"steps\":np.ceil(train_size/30), \"val_steps\": np.ceil(val_size/30), \"optimizer\": tf.optimizers.Adagrad(learning_rate=0.0001),\"epochs\": 10},\n",
    "                {\"batch\": 32,\"steps\":np.ceil(train_size/32), \"val_steps\": np.ceil(val_size/32), \"optimizer\": tf.optimizers.Adam(learning_rate=0.00001),\"epochs\": 10},\n",
    "                {\"batch\": 35,\"steps\":np.ceil(train_size/35), \"val_steps\": np.ceil(val_size/35), \"optimizer\": tf.optimizers.Adam(learning_rate=0.0001),\"epochs\": 10}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "J6W2TLLsdf7y",
    "outputId": "bc449988-41fc-4ee0-e184-f9161f693f41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 150, 150, 128)     3584      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 150, 150, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 75, 75, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 75, 75, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 37, 37, 128)       295040    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 175232)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               22429824  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 102)               13158     \n",
      "=================================================================\n",
      "Total params: 23,774,438\n",
      "Trainable params: 23,774,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " \n",
      " ------------------------------------------------\n",
      "\n",
      " Training for hyperparameter 0 and model 3 .... \n",
      " \n",
      "\n",
      " train size: 29336, validation size: 6769, batch size: 25, epochs: 10 \n",
      "\n",
      "------------------------------------------------ \n",
      " \n",
      "\n",
      "Train for 1174.0 steps, validate for 271.0 steps\n",
      "Epoch 1/10\n",
      "1174/1174 [==============================] - 173s 148ms/step - loss: 4.1811 - accuracy: 0.0827 - categorical_accuracy: 0.0827 - val_loss: 4.1025 - val_accuracy: 0.1002 - val_categorical_accuracy: 0.1002\n",
      "Epoch 2/10\n",
      "1174/1174 [==============================] - 151s 129ms/step - loss: 4.0175 - accuracy: 0.1103 - categorical_accuracy: 0.1103 - val_loss: 3.9201 - val_accuracy: 0.1296 - val_categorical_accuracy: 0.1296\n",
      "Epoch 3/10\n",
      "1174/1174 [==============================] - 151s 129ms/step - loss: 3.8497 - accuracy: 0.1385 - categorical_accuracy: 0.1385 - val_loss: 3.7611 - val_accuracy: 0.1585 - val_categorical_accuracy: 0.1585\n",
      "Epoch 4/10\n",
      "1174/1174 [==============================] - 151s 129ms/step - loss: 3.6995 - accuracy: 0.1619 - categorical_accuracy: 0.1619 - val_loss: 3.6179 - val_accuracy: 0.1841 - val_categorical_accuracy: 0.1841\n",
      "Epoch 5/10\n",
      "1174/1174 [==============================] - 151s 129ms/step - loss: 3.5601 - accuracy: 0.1858 - categorical_accuracy: 0.1858 - val_loss: 3.5129 - val_accuracy: 0.2111 - val_categorical_accuracy: 0.2111\n",
      "Epoch 6/10\n",
      "1174/1174 [==============================] - 151s 129ms/step - loss: 3.4068 - accuracy: 0.2142 - categorical_accuracy: 0.2142 - val_loss: 3.2983 - val_accuracy: 0.2528 - val_categorical_accuracy: 0.2528\n",
      "Epoch 7/10\n",
      "1174/1174 [==============================] - 152s 129ms/step - loss: 3.2346 - accuracy: 0.2425 - categorical_accuracy: 0.2425 - val_loss: 3.1258 - val_accuracy: 0.2952 - val_categorical_accuracy: 0.2952\n",
      "Epoch 8/10\n",
      "1174/1174 [==============================] - 152s 130ms/step - loss: 3.0133 - accuracy: 0.2845 - categorical_accuracy: 0.2845 - val_loss: 2.9441 - val_accuracy: 0.3556 - val_categorical_accuracy: 0.3556\n",
      "Epoch 9/10\n",
      "1174/1174 [==============================] - 152s 129ms/step - loss: 2.6865 - accuracy: 0.3484 - categorical_accuracy: 0.3484 - val_loss: 2.6937 - val_accuracy: 0.3780 - val_categorical_accuracy: 0.3780\n",
      "Epoch 10/10\n",
      "1174/1174 [==============================] - 152s 129ms/step - loss: 2.2630 - accuracy: 0.4297 - categorical_accuracy: 0.4297 - val_loss: 2.2061 - val_accuracy: 0.5142 - val_categorical_accuracy: 0.5142\n",
      "Keras model saved as: model_log_hp_0_md_3.h5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: model_log_hp_0_md_3/assets\n",
      "SavedModel file saved under: model_log_hp_0_md_3\n",
      "History file saved as history_log_hp_0_md_3\n",
      "SavedModel file zipped as: model_log_hp_0_md_3\n",
      "File model_log_hp_0_md_3.h5 uploaded to Google Bucket: training_logs/data_split_4/model_log_hp_0_md_3.h5.\n",
      "File history_log_hp_0_md_3 uploaded to Google Bucket: training_logs/data_split_4/history_log_hp_0_md_3.\n",
      "File model_log_hp_0_md_3.zip uploaded to Google Bucket: training_logs/data_split_4/model_log_hp_0_md_3.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf_no_top.h5\n",
      "17227776/17225924 [==============================] - 2s 0us/step\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenet_1.00_224 (Model)   (None, 4, 4, 1024)        3228864   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               4194560   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 102)               26214     \n",
      "=================================================================\n",
      "Total params: 7,449,638\n",
      "Trainable params: 7,427,750\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n",
      "\n",
      " \n",
      " ------------------------------------------------\n",
      "\n",
      " Training for hyperparameter 0 and model 4 .... \n",
      " \n",
      "\n",
      " train size: 29336, validation size: 6769, batch size: 25, epochs: 10 \n",
      "\n",
      "------------------------------------------------ \n",
      " \n",
      "\n",
      "Train for 1174.0 steps, validate for 271.0 steps\n",
      "Epoch 1/10\n",
      "1174/1174 [==============================] - 73s 62ms/step - loss: 3.4926 - accuracy: 0.2496 - categorical_accuracy: 0.2496 - val_loss: 2.5758 - val_accuracy: 0.4015 - val_categorical_accuracy: 0.4015\n",
      "Epoch 2/10\n",
      "1174/1174 [==============================] - 69s 59ms/step - loss: 2.6213 - accuracy: 0.3838 - categorical_accuracy: 0.3838 - val_loss: 2.0592 - val_accuracy: 0.5051 - val_categorical_accuracy: 0.5051\n",
      "Epoch 3/10\n",
      "1174/1174 [==============================] - 69s 59ms/step - loss: 2.1024 - accuracy: 0.4780 - categorical_accuracy: 0.4780 - val_loss: 1.7256 - val_accuracy: 0.5665 - val_categorical_accuracy: 0.5665\n",
      "Epoch 4/10\n",
      "1174/1174 [==============================] - 70s 59ms/step - loss: 1.7025 - accuracy: 0.5561 - categorical_accuracy: 0.5561 - val_loss: 1.4596 - val_accuracy: 0.6363 - val_categorical_accuracy: 0.6363\n",
      "Epoch 5/10\n",
      "1174/1174 [==============================] - 69s 59ms/step - loss: 1.4142 - accuracy: 0.6207 - categorical_accuracy: 0.6207 - val_loss: 1.3627 - val_accuracy: 0.6701 - val_categorical_accuracy: 0.6701\n",
      "Epoch 6/10\n",
      "1174/1174 [==============================] - 69s 59ms/step - loss: 1.1550 - accuracy: 0.6767 - categorical_accuracy: 0.6767 - val_loss: 1.2658 - val_accuracy: 0.6993 - val_categorical_accuracy: 0.6993\n",
      "Epoch 7/10\n",
      "1174/1174 [==============================] - 69s 59ms/step - loss: 0.9617 - accuracy: 0.7306 - categorical_accuracy: 0.7306 - val_loss: 1.2540 - val_accuracy: 0.7147 - val_categorical_accuracy: 0.7147\n",
      "Epoch 8/10\n",
      "1174/1174 [==============================] - 69s 59ms/step - loss: 0.8000 - accuracy: 0.7744 - categorical_accuracy: 0.7744 - val_loss: 1.1146 - val_accuracy: 0.7504 - val_categorical_accuracy: 0.7504\n",
      "Epoch 9/10\n",
      "1173/1174 [============================>.] - ETA: 0s - loss: 0.6773 - accuracy: 0.8106 - categorical_accuracy: 0.8106\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
      "1174/1174 [==============================] - 70s 59ms/step - loss: 0.6775 - accuracy: 0.8106 - categorical_accuracy: 0.8106 - val_loss: 1.1244 - val_accuracy: 0.7621 - val_categorical_accuracy: 0.7621\n",
      "Epoch 10/10\n",
      "1174/1174 [==============================] - 69s 59ms/step - loss: 0.4020 - accuracy: 0.8948 - categorical_accuracy: 0.8948 - val_loss: 0.9246 - val_accuracy: 0.8310 - val_categorical_accuracy: 0.8310\n",
      "Keras model saved as: model_log_hp_0_md_4.h5\n",
      "INFO:tensorflow:Assets written to: model_log_hp_0_md_4/assets\n",
      "SavedModel file saved under: model_log_hp_0_md_4\n",
      "History file saved as history_log_hp_0_md_4\n",
      "SavedModel file zipped as: model_log_hp_0_md_4\n",
      "File model_log_hp_0_md_4.h5 uploaded to Google Bucket: training_logs/data_split_4/model_log_hp_0_md_4.h5.\n",
      "File history_log_hp_0_md_4 uploaded to Google Bucket: training_logs/data_split_4/history_log_hp_0_md_4.\n",
      "File model_log_hp_0_md_4.zip uploaded to Google Bucket: training_logs/data_split_4/model_log_hp_0_md_4.zip.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 150, 150, 128)     3584      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 150, 150, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 75, 75, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 75, 75, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 37, 37, 128)       295040    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 175232)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               22429824  \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 102)               13158     \n",
      "=================================================================\n",
      "Total params: 23,774,438\n",
      "Trainable params: 23,774,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " \n",
      " ------------------------------------------------\n",
      "\n",
      " Training for hyperparameter 1 and model 3 .... \n",
      " \n",
      "\n",
      " train size: 29336, validation size: 6769, batch size: 28, epochs: 10 \n",
      "\n",
      "------------------------------------------------ \n",
      " \n",
      "\n",
      "Train for 1048.0 steps, validate for 242.0 steps\n",
      "Epoch 1/10\n",
      "1048/1048 [==============================] - 164s 156ms/step - loss: 4.3478 - accuracy: 0.0787 - categorical_accuracy: 0.0787 - val_loss: 4.0662 - val_accuracy: 0.1145 - val_categorical_accuracy: 0.1145\n",
      "Epoch 2/10\n",
      "1048/1048 [==============================] - 148s 141ms/step - loss: 4.0064 - accuracy: 0.1237 - categorical_accuracy: 0.1237 - val_loss: 3.8409 - val_accuracy: 0.1328 - val_categorical_accuracy: 0.1328\n",
      "Epoch 3/10\n",
      "1048/1048 [==============================] - 149s 142ms/step - loss: 3.8484 - accuracy: 0.1419 - categorical_accuracy: 0.1419 - val_loss: 3.7005 - val_accuracy: 0.1566 - val_categorical_accuracy: 0.1566\n",
      "Epoch 4/10\n",
      "1048/1048 [==============================] - 149s 142ms/step - loss: 3.7246 - accuracy: 0.1577 - categorical_accuracy: 0.1577 - val_loss: 3.6247 - val_accuracy: 0.1710 - val_categorical_accuracy: 0.1710\n",
      "Epoch 5/10\n",
      "1048/1048 [==============================] - 149s 143ms/step - loss: 3.6225 - accuracy: 0.1700 - categorical_accuracy: 0.1700 - val_loss: 3.5644 - val_accuracy: 0.1919 - val_categorical_accuracy: 0.1919\n",
      "Epoch 6/10\n",
      "1048/1048 [==============================] - 149s 142ms/step - loss: 3.5560 - accuracy: 0.1848 - categorical_accuracy: 0.1848 - val_loss: 3.4801 - val_accuracy: 0.1989 - val_categorical_accuracy: 0.1989\n",
      "Epoch 7/10\n",
      "1047/1048 [============================>.] - ETA: 0s - loss: 3.4906 - accuracy: 0.1986 - categorical_accuracy: 0.1986\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "1048/1048 [==============================] - 149s 142ms/step - loss: 3.4905 - accuracy: 0.1986 - categorical_accuracy: 0.1986 - val_loss: 3.4998 - val_accuracy: 0.1985 - val_categorical_accuracy: 0.1985\n",
      "Epoch 8/10\n",
      "1048/1048 [==============================] - 148s 141ms/step - loss: 3.2554 - accuracy: 0.2289 - categorical_accuracy: 0.2289 - val_loss: 3.3040 - val_accuracy: 0.2354 - val_categorical_accuracy: 0.2354\n",
      "Epoch 9/10\n",
      "1047/1048 [============================>.] - ETA: 0s - loss: 3.2018 - accuracy: 0.2409 - categorical_accuracy: 0.2409\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.\n",
      "1048/1048 [==============================] - 148s 141ms/step - loss: 3.2015 - accuracy: 0.2409 - categorical_accuracy: 0.2409 - val_loss: 3.3279 - val_accuracy: 0.2392 - val_categorical_accuracy: 0.2392\n",
      "Epoch 10/10\n",
      "1047/1048 [============================>.] - ETA: 0s - loss: 3.0806 - accuracy: 0.2586 - categorical_accuracy: 0.2586\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n",
      "1048/1048 [==============================] - 148s 141ms/step - loss: 3.0809 - accuracy: 0.2586 - categorical_accuracy: 0.2586 - val_loss: 3.4035 - val_accuracy: 0.2386 - val_categorical_accuracy: 0.2386\n",
      "Keras model saved as: model_log_hp_1_md_3.h5\n",
      "INFO:tensorflow:Assets written to: model_log_hp_1_md_3/assets\n",
      "SavedModel file saved under: model_log_hp_1_md_3\n",
      "History file saved as history_log_hp_1_md_3\n",
      "SavedModel file zipped as: model_log_hp_1_md_3\n",
      "File model_log_hp_1_md_3.h5 uploaded to Google Bucket: training_logs/data_split_4/model_log_hp_1_md_3.h5.\n",
      "File history_log_hp_1_md_3 uploaded to Google Bucket: training_logs/data_split_4/history_log_hp_1_md_3.\n",
      "File model_log_hp_1_md_3.zip uploaded to Google Bucket: training_logs/data_split_4/model_log_hp_1_md_3.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenet_1.00_224 (Model)   (None, 4, 4, 1024)        3228864   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               4194560   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 102)               26214     \n",
      "=================================================================\n",
      "Total params: 7,449,638\n",
      "Trainable params: 7,427,750\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n",
      "\n",
      " \n",
      " ------------------------------------------------\n",
      "\n",
      " Training for hyperparameter 1 and model 4 .... \n",
      " \n",
      "\n",
      " train size: 29336, validation size: 6769, batch size: 28, epochs: 10 \n",
      "\n",
      "------------------------------------------------ \n",
      " \n",
      "\n",
      "Train for 1048.0 steps, validate for 242.0 steps\n",
      "Epoch 1/10\n",
      "1048/1048 [==============================] - 76s 72ms/step - loss: 3.5560 - accuracy: 0.2508 - categorical_accuracy: 0.2508 - val_loss: 2.4064 - val_accuracy: 0.4416 - val_categorical_accuracy: 0.4416\n",
      "Epoch 2/10\n",
      "1048/1048 [==============================] - 72s 68ms/step - loss: 2.3716 - accuracy: 0.4486 - categorical_accuracy: 0.4486 - val_loss: 1.7660 - val_accuracy: 0.5831 - val_categorical_accuracy: 0.5831\n",
      "Epoch 3/10\n",
      "1048/1048 [==============================] - 72s 68ms/step - loss: 1.6894 - accuracy: 0.5845 - categorical_accuracy: 0.5845 - val_loss: 1.4765 - val_accuracy: 0.6619 - val_categorical_accuracy: 0.6619\n",
      "Epoch 4/10\n",
      "1048/1048 [==============================] - 72s 68ms/step - loss: 1.1743 - accuracy: 0.6968 - categorical_accuracy: 0.6968 - val_loss: 1.2477 - val_accuracy: 0.7336 - val_categorical_accuracy: 0.7336\n",
      "Epoch 5/10\n",
      "1048/1048 [==============================] - 72s 69ms/step - loss: 0.8153 - accuracy: 0.7848 - categorical_accuracy: 0.7848 - val_loss: 1.1712 - val_accuracy: 0.7618 - val_categorical_accuracy: 0.7618\n",
      "Epoch 6/10\n",
      "1048/1048 [==============================] - 72s 68ms/step - loss: 0.5931 - accuracy: 0.8416 - categorical_accuracy: 0.8416 - val_loss: 1.1557 - val_accuracy: 0.7848 - val_categorical_accuracy: 0.7848\n",
      "Epoch 7/10\n",
      "1048/1048 [==============================] - 72s 68ms/step - loss: 0.4636 - accuracy: 0.8780 - categorical_accuracy: 0.8780 - val_loss: 1.1054 - val_accuracy: 0.8017 - val_categorical_accuracy: 0.8017\n",
      "Epoch 8/10\n",
      "1047/1048 [============================>.] - ETA: 0s - loss: 0.3708 - accuracy: 0.9058 - categorical_accuracy: 0.9058\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.\n",
      "1048/1048 [==============================] - 71s 68ms/step - loss: 0.3706 - accuracy: 0.9059 - categorical_accuracy: 0.9059 - val_loss: 1.2986 - val_accuracy: 0.8014 - val_categorical_accuracy: 0.8014\n",
      "Epoch 9/10\n",
      "1047/1048 [============================>.] - ETA: 0s - loss: 0.2229 - accuracy: 0.9505 - categorical_accuracy: 0.9505\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0240000847261399e-05.\n",
      "1048/1048 [==============================] - 72s 68ms/step - loss: 0.2228 - accuracy: 0.9505 - categorical_accuracy: 0.9505 - val_loss: 1.1169 - val_accuracy: 0.8341 - val_categorical_accuracy: 0.8341\n",
      "Epoch 10/10\n",
      "1047/1048 [============================>.] - ETA: 0s - loss: 0.1499 - accuracy: 0.9721 - categorical_accuracy: 0.9721\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.09600033890456e-06.\n",
      "1048/1048 [==============================] - 72s 68ms/step - loss: 0.1499 - accuracy: 0.9721 - categorical_accuracy: 0.9721 - val_loss: 1.1123 - val_accuracy: 0.8393 - val_categorical_accuracy: 0.8393\n",
      "Keras model saved as: model_log_hp_1_md_4.h5\n",
      "INFO:tensorflow:Assets written to: model_log_hp_1_md_4/assets\n",
      "SavedModel file saved under: model_log_hp_1_md_4\n",
      "History file saved as history_log_hp_1_md_4\n",
      "SavedModel file zipped as: model_log_hp_1_md_4\n",
      "File model_log_hp_1_md_4.h5 uploaded to Google Bucket: training_logs/data_split_4/model_log_hp_1_md_4.h5.\n",
      "File history_log_hp_1_md_4 uploaded to Google Bucket: training_logs/data_split_4/history_log_hp_1_md_4.\n",
      "File model_log_hp_1_md_4.zip uploaded to Google Bucket: training_logs/data_split_4/model_log_hp_1_md_4.zip.\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 150, 150, 128)     3584      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 150, 150, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 75, 75, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 75, 75, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 37, 37, 128)       295040    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 175232)            0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               22429824  \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 102)               13158     \n",
      "=================================================================\n",
      "Total params: 23,774,438\n",
      "Trainable params: 23,774,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " \n",
      " ------------------------------------------------\n",
      "\n",
      " Training for hyperparameter 2 and model 3 .... \n",
      " \n",
      "\n",
      " train size: 29336, validation size: 6769, batch size: 30, epochs: 10 \n",
      "\n",
      "------------------------------------------------ \n",
      " \n",
      "\n",
      "Train for 978.0 steps, validate for 226.0 steps\n",
      "Epoch 1/10\n",
      "978/978 [==============================] - 157s 161ms/step - loss: 4.4220 - accuracy: 0.0738 - categorical_accuracy: 0.0738 - val_loss: 4.3020 - val_accuracy: 0.0729 - val_categorical_accuracy: 0.0729\n",
      "Epoch 2/10\n",
      "978/978 [==============================] - 144s 147ms/step - loss: 4.2970 - accuracy: 0.0709 - categorical_accuracy: 0.0709 - val_loss: 4.2728 - val_accuracy: 0.0748 - val_categorical_accuracy: 0.0748\n",
      "Epoch 3/10\n",
      "978/978 [==============================] - 143s 147ms/step - loss: 4.2787 - accuracy: 0.0736 - categorical_accuracy: 0.0736 - val_loss: 4.2592 - val_accuracy: 0.0748 - val_categorical_accuracy: 0.0748\n",
      "Epoch 4/10\n",
      "978/978 [==============================] - 143s 147ms/step - loss: 4.2689 - accuracy: 0.0727 - categorical_accuracy: 0.0727 - val_loss: 4.2504 - val_accuracy: 0.0748 - val_categorical_accuracy: 0.0748\n",
      "Epoch 5/10\n",
      "978/978 [==============================] - 143s 146ms/step - loss: 4.2548 - accuracy: 0.0737 - categorical_accuracy: 0.0737 - val_loss: 4.2406 - val_accuracy: 0.0745 - val_categorical_accuracy: 0.0745\n",
      "Epoch 6/10\n",
      "978/978 [==============================] - 142s 146ms/step - loss: 4.2454 - accuracy: 0.0778 - categorical_accuracy: 0.0778 - val_loss: 4.2311 - val_accuracy: 0.0760 - val_categorical_accuracy: 0.0760\n",
      "Epoch 7/10\n",
      "978/978 [==============================] - 142s 146ms/step - loss: 4.2403 - accuracy: 0.0752 - categorical_accuracy: 0.0752 - val_loss: 4.2240 - val_accuracy: 0.0754 - val_categorical_accuracy: 0.0754\n",
      "Epoch 8/10\n",
      "842/978 [========================>.....] - ETA: 18s - loss: 4.2318 - accuracy: 0.0760 - categorical_accuracy: 0.0760Buffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "for i, hparam in enumerate(hypers):\n",
    "\n",
    "  train_h = train_ds.cache() # cache the dataset in RAM or on disk\n",
    "  train_h = train_h.repeat() # repeat the dataset indefinitely\n",
    "  train_h = train_h.batch(hparam[\"batch\"]) # batch data elements together in batches of 128\n",
    "  train_h = train_h.prefetch(AUTOTUNE)\n",
    "\n",
    "  val_h = val_ds.cache() # cache the dataset in RAM or on disk\n",
    "  val_h = val_h.repeat() # repeat the dataset indefinitely\n",
    "  val_h = val_h.batch(hparam[\"batch\"]) # batch data elements together in batches of 128\n",
    "  val_h = val_h.prefetch(AUTOTUNE)\n",
    "\n",
    "  for j in range(0,5):\n",
    "    \n",
    "    if j == 0:\n",
    "      continue\n",
    "      model = create_model_1()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 1:\n",
    "      continue\n",
    "      model = create_model_2()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 2:\n",
    "      continue\n",
    "      model = create_model_3()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 3:\n",
    "      model = create_model_4()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 4:\n",
    "      model = create_model_5()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.4, verbose=1, patience=1, min_lr=0.0000001)\n",
    "\n",
    "    print(\"\\n \\n ------------------------------------------------\")\n",
    "    print(\"\\n Training for hyperparameter {} and model {} .... \\n \".format(i,j))\n",
    "    print(\"\\n train size: {}, validation size: {}, batch size: {}, epochs: 10 \\n\".format(train_size, val_size, hparam[\"batch\"]))\n",
    "    print(\"------------------------------------------------ \\n \\n\")\n",
    "\n",
    "    history = model.fit(train_h, epochs=hparam[\"epochs\"], validation_data=val_h, steps_per_epoch=hparam[\"steps\"], validation_steps=hparam[\"val_steps\"], callbacks=[reduce_lr])\n",
    "    \n",
    "    model.save(\"model_log_hp_{}_md_{}.h5\".format(i,j))\n",
    "    print(\"Keras model saved as: model_log_hp_{}_md_{}.h5\".format(i,j))\n",
    "    model.save('model_log_hp_{}_md_{}'.format(i,j), save_format='tf')\n",
    "    print(\"SavedModel file saved under: model_log_hp_{}_md_{}\".format(i,j))\n",
    "\n",
    "    with open('history_log_hp_{}_md_{}'.format(i,j), 'wb') as file_pi:\n",
    "      pickle.dump(history.history, file_pi)\n",
    "    print(\"History file saved as history_log_hp_{}_md_{}\".format(i,j))\n",
    "\n",
    "    zipf = zipfile.ZipFile('model_log_hp_{}_md_{}.zip'.format(i,j), 'w', zipfile.ZIP_DEFLATED)\n",
    "    zipdir('model_log_hp_{}_md_{}'.format(i,j), zipf)\n",
    "    zipf.close()\n",
    "    print(\"SavedModel file zipped as: model_log_hp_{}_md_{}\".format(i,j))\n",
    "    \n",
    "    upload_blob(\"model_log_hp_{}_md_{}.h5\".format(i,j), choice)\n",
    "    upload_blob(\"history_log_hp_{}_md_{}\".format(i,j), choice)\n",
    "    upload_blob(\"model_log_hp_{}_md_{}.zip\".format(i,j), choice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFeX0pNYdmOT"
   },
   "outputs": [],
   "source": [
    "# a = []\n",
    "# while(1):\n",
    "#   a.append('1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CpyGbHk4X-u"
   },
   "source": [
    "## Data Split: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNhmnzzksCDE"
   },
   "outputs": [],
   "source": [
    "choice = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "9ZZ8mPSM4d2j",
    "outputId": "bed29471-6b9c-41bc-cf2e-e86f306aa6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31593 4514 9026\n",
      "total:  45133\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds, train_size, test_size, val_size = data_split(full_ds, dataset_size, choice=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQ-InS094gZg"
   },
   "outputs": [],
   "source": [
    "hypers = [{\"batch\": 25,\"steps\":np.ceil(train_size/25), \"val_steps\": np.ceil(val_size/25),\"optimizer\": tf.optimizers.SGD(learning_rate=0.01),\"epochs\": 10},\n",
    "                {\"batch\": 28,\"steps\":np.ceil(train_size/28), \"val_steps\": np.ceil(val_size/28), \"optimizer\": tf.optimizers.RMSprop(learning_rate=0.001),\"epochs\": 10},\n",
    "                {\"batch\": 30,\"steps\":np.ceil(train_size/30), \"val_steps\": np.ceil(val_size/30), \"optimizer\": tf.optimizers.Adagrad(learning_rate=0.0001),\"epochs\": 10},\n",
    "                {\"batch\": 32,\"steps\":np.ceil(train_size/32), \"val_steps\": np.ceil(val_size/32), \"optimizer\": tf.optimizers.Adam(learning_rate=0.00001),\"epochs\": 10},\n",
    "                {\"batch\": 35,\"steps\":np.ceil(train_size/35), \"val_steps\": np.ceil(val_size/35), \"optimizer\": tf.optimizers.Adam(learning_rate=0.0001),\"epochs\": 10}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Nll0RuD_4jXS",
    "outputId": "f187b4b3-d051-460f-b203-ad09d1a3162b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 150, 150, 128)     3584      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 150, 150, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 75, 75, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 75, 75, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 37, 37, 128)       295040    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 175232)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               22429824  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 102)               13158     \n",
      "=================================================================\n",
      "Total params: 23,774,438\n",
      "Trainable params: 23,774,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " \n",
      " ------------------------------------------------\n",
      "\n",
      " Training for hyperparameter 4 and model 3 .... \n",
      " \n",
      "\n",
      " train size: 31593, validation size: 9026, batch size: 35, epochs: 10 \n",
      "\n",
      "------------------------------------------------ \n",
      " \n",
      "\n",
      "Train for 903.0 steps, validate for 258.0 steps\n",
      "Epoch 1/10\n",
      "903/903 [==============================] - 210s 233ms/step - loss: 4.1591 - accuracy: 0.1008 - categorical_accuracy: 0.1008 - val_loss: 3.9358 - val_accuracy: 0.1281 - val_categorical_accuracy: 0.1281\n",
      "Epoch 2/10\n",
      "903/903 [==============================] - 173s 192ms/step - loss: 3.8886 - accuracy: 0.1409 - categorical_accuracy: 0.1409 - val_loss: 3.7192 - val_accuracy: 0.1649 - val_categorical_accuracy: 0.1649\n",
      "Epoch 3/10\n",
      "903/903 [==============================] - 173s 192ms/step - loss: 3.6586 - accuracy: 0.1762 - categorical_accuracy: 0.1762 - val_loss: 3.4727 - val_accuracy: 0.2132 - val_categorical_accuracy: 0.2132\n",
      "Epoch 4/10\n",
      "903/903 [==============================] - 173s 192ms/step - loss: 3.3636 - accuracy: 0.2277 - categorical_accuracy: 0.2277 - val_loss: 3.2408 - val_accuracy: 0.2651 - val_categorical_accuracy: 0.2651\n",
      "Epoch 5/10\n",
      "903/903 [==============================] - 173s 192ms/step - loss: 2.9662 - accuracy: 0.2979 - categorical_accuracy: 0.2979 - val_loss: 2.9888 - val_accuracy: 0.3293 - val_categorical_accuracy: 0.3293\n",
      "Epoch 6/10\n",
      "903/903 [==============================] - 174s 192ms/step - loss: 2.5207 - accuracy: 0.3795 - categorical_accuracy: 0.3795 - val_loss: 2.7438 - val_accuracy: 0.3897 - val_categorical_accuracy: 0.3897\n",
      "Epoch 7/10\n",
      "903/903 [==============================] - 173s 192ms/step - loss: 2.1100 - accuracy: 0.4629 - categorical_accuracy: 0.4629 - val_loss: 2.5786 - val_accuracy: 0.4330 - val_categorical_accuracy: 0.4330\n",
      "Epoch 8/10\n",
      "903/903 [==============================] - 173s 192ms/step - loss: 1.7400 - accuracy: 0.5390 - categorical_accuracy: 0.5390 - val_loss: 2.4178 - val_accuracy: 0.4734 - val_categorical_accuracy: 0.4734\n",
      "Epoch 9/10\n",
      "903/903 [==============================] - 173s 192ms/step - loss: 1.4233 - accuracy: 0.6112 - categorical_accuracy: 0.6112 - val_loss: 2.2124 - val_accuracy: 0.5399 - val_categorical_accuracy: 0.5399\n",
      "Epoch 10/10\n",
      "903/903 [==============================] - 173s 192ms/step - loss: 1.2051 - accuracy: 0.6625 - categorical_accuracy: 0.6625 - val_loss: 2.0505 - val_accuracy: 0.5884 - val_categorical_accuracy: 0.5884\n",
      "Keras model saved as: model_log_hp_4_md_3.h5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: model_log_hp_4_md_3/assets\n",
      "SavedModel file saved under: model_log_hp_4_md_3\n",
      "History file saved as history_log_hp_4_md_3\n",
      "SavedModel file zipped as: model_log_hp_4_md_3\n",
      "File model_log_hp_4_md_3.h5 uploaded to Google Bucket: training_logs/data_split_5/model_log_hp_4_md_3.h5.\n",
      "File history_log_hp_4_md_3 uploaded to Google Bucket: training_logs/data_split_5/history_log_hp_4_md_3.\n",
      "File model_log_hp_4_md_3.zip uploaded to Google Bucket: training_logs/data_split_5/model_log_hp_4_md_3.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf_no_top.h5\n",
      "17227776/17225924 [==============================] - 0s 0us/step\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenet_1.00_224 (Model)   (None, 4, 4, 1024)        3228864   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               4194560   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 102)               26214     \n",
      "=================================================================\n",
      "Total params: 7,449,638\n",
      "Trainable params: 7,427,750\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n",
      "\n",
      " \n",
      " ------------------------------------------------\n",
      "\n",
      " Training for hyperparameter 4 and model 4 .... \n",
      " \n",
      "\n",
      " train size: 31593, validation size: 9026, batch size: 35, epochs: 10 \n",
      "\n",
      "------------------------------------------------ \n",
      " \n",
      "\n",
      "Train for 903.0 steps, validate for 258.0 steps\n",
      "Epoch 1/10\n",
      "903/903 [==============================] - 78s 87ms/step - loss: 3.6504 - accuracy: 0.2290 - categorical_accuracy: 0.2290 - val_loss: 2.8133 - val_accuracy: 0.3697 - val_categorical_accuracy: 0.3697\n",
      "Epoch 2/10\n",
      "903/903 [==============================] - 76s 84ms/step - loss: 2.7619 - accuracy: 0.3675 - categorical_accuracy: 0.3675 - val_loss: 2.1836 - val_accuracy: 0.4942 - val_categorical_accuracy: 0.4942\n",
      "Epoch 3/10\n",
      "903/903 [==============================] - 76s 84ms/step - loss: 2.1997 - accuracy: 0.4617 - categorical_accuracy: 0.4617 - val_loss: 1.7696 - val_accuracy: 0.5783 - val_categorical_accuracy: 0.5783\n",
      "Epoch 4/10\n",
      "903/903 [==============================] - 76s 84ms/step - loss: 1.7470 - accuracy: 0.5532 - categorical_accuracy: 0.5532 - val_loss: 1.4732 - val_accuracy: 0.6519 - val_categorical_accuracy: 0.6519\n",
      "Epoch 5/10\n",
      "903/903 [==============================] - 76s 84ms/step - loss: 1.4040 - accuracy: 0.6249 - categorical_accuracy: 0.6249 - val_loss: 1.2657 - val_accuracy: 0.6896 - val_categorical_accuracy: 0.6896\n",
      "Epoch 6/10\n",
      "903/903 [==============================] - 76s 84ms/step - loss: 1.1202 - accuracy: 0.6929 - categorical_accuracy: 0.6929 - val_loss: 1.1170 - val_accuracy: 0.7406 - val_categorical_accuracy: 0.7406\n",
      "Epoch 7/10\n",
      "903/903 [==============================] - 76s 84ms/step - loss: 0.9071 - accuracy: 0.7481 - categorical_accuracy: 0.7481 - val_loss: 1.0176 - val_accuracy: 0.7697 - val_categorical_accuracy: 0.7697\n",
      "Epoch 8/10\n",
      "903/903 [==============================] - 75s 83ms/step - loss: 0.7363 - accuracy: 0.7930 - categorical_accuracy: 0.7930 - val_loss: 0.9706 - val_accuracy: 0.7893 - val_categorical_accuracy: 0.7893\n",
      "Epoch 9/10\n",
      "903/903 [==============================] - 75s 83ms/step - loss: 0.6219 - accuracy: 0.8232 - categorical_accuracy: 0.8232 - val_loss: 0.9295 - val_accuracy: 0.8092 - val_categorical_accuracy: 0.8092\n",
      "Epoch 10/10\n",
      "903/903 [==============================] - 76s 84ms/step - loss: 0.5287 - accuracy: 0.8486 - categorical_accuracy: 0.8486 - val_loss: 0.8995 - val_accuracy: 0.8205 - val_categorical_accuracy: 0.8205\n",
      "Keras model saved as: model_log_hp_4_md_4.h5\n",
      "INFO:tensorflow:Assets written to: model_log_hp_4_md_4/assets\n",
      "SavedModel file saved under: model_log_hp_4_md_4\n",
      "History file saved as history_log_hp_4_md_4\n",
      "SavedModel file zipped as: model_log_hp_4_md_4\n",
      "File model_log_hp_4_md_4.h5 uploaded to Google Bucket: training_logs/data_split_5/model_log_hp_4_md_4.h5.\n",
      "File history_log_hp_4_md_4 uploaded to Google Bucket: training_logs/data_split_5/history_log_hp_4_md_4.\n",
      "File model_log_hp_4_md_4.zip uploaded to Google Bucket: training_logs/data_split_5/model_log_hp_4_md_4.zip.\n"
     ]
    }
   ],
   "source": [
    "for i, hparam in enumerate(hypers):\n",
    "\n",
    "  if i == 0 or i == 1 or i == 2 or i == 3:\n",
    "    continue\n",
    "\n",
    "  train_h = train_ds.cache() # cache the dataset in RAM or on disk\n",
    "  train_h = train_h.repeat() # repeat the dataset indefinitely\n",
    "  train_h = train_h.batch(hparam[\"batch\"]) # batch data elements together in batches of 128\n",
    "  train_h = train_h.prefetch(AUTOTUNE)\n",
    "\n",
    "  val_h = val_ds.cache() # cache the dataset in RAM or on disk\n",
    "  val_h = val_h.repeat() # repeat the dataset indefinitely\n",
    "  val_h = val_h.batch(hparam[\"batch\"]) # batch data elements together in batches of 128\n",
    "  val_h = val_h.prefetch(AUTOTUNE)\n",
    "\n",
    "  for j in range(0,5):\n",
    "    \n",
    "    if j == 0:\n",
    "      continue\n",
    "      model = create_model_1()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 1:\n",
    "      continue\n",
    "      model = create_model_2()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 2:\n",
    "      continue\n",
    "      model = create_model_3()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 3:\n",
    "      model = create_model_4()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    if j == 4:\n",
    "      model = create_model_5()\n",
    "      model.summary()\n",
    "      model.compile(optimizer=hparam[\"optimizer\"], loss=tf.losses.CategoricalCrossentropy(),metrics=['accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.4, verbose=1, patience=1, min_lr=0.0000001)\n",
    "\n",
    "    print(\"\\n \\n ------------------------------------------------\")\n",
    "    print(\"\\n Training for hyperparameter {} and model {} .... \\n \".format(i,j))\n",
    "    print(\"\\n train size: {}, validation size: {}, batch size: {}, epochs: 10 \\n\".format(train_size, val_size, hparam[\"batch\"]))\n",
    "    print(\"------------------------------------------------ \\n \\n\")\n",
    "\n",
    "    history = model.fit(train_h, epochs=hparam[\"epochs\"], validation_data=val_h, steps_per_epoch=hparam[\"steps\"], validation_steps=hparam[\"val_steps\"], callbacks=[reduce_lr])\n",
    "    \n",
    "    model.save(\"model_log_hp_{}_md_{}.h5\".format(i,j))\n",
    "    print(\"Keras model saved as: model_log_hp_{}_md_{}.h5\".format(i,j))\n",
    "    model.save('model_log_hp_{}_md_{}'.format(i,j), save_format='tf')\n",
    "    print(\"SavedModel file saved under: model_log_hp_{}_md_{}\".format(i,j))\n",
    "\n",
    "    with open('history_log_hp_{}_md_{}'.format(i,j), 'wb') as file_pi:\n",
    "      pickle.dump(history.history, file_pi)\n",
    "    print(\"History file saved as history_log_hp_{}_md_{}\".format(i,j))\n",
    "\n",
    "    zipf = zipfile.ZipFile('model_log_hp_{}_md_{}.zip'.format(i,j), 'w', zipfile.ZIP_DEFLATED)\n",
    "    zipdir('model_log_hp_{}_md_{}'.format(i,j), zipf)\n",
    "    zipf.close()\n",
    "    print(\"SavedModel file zipped as: model_log_hp_{}_md_{}\".format(i,j))\n",
    "    \n",
    "    upload_blob(\"model_log_hp_{}_md_{}.h5\".format(i,j), choice)\n",
    "    upload_blob(\"history_log_hp_{}_md_{}\".format(i,j), choice)\n",
    "    upload_blob(\"model_log_hp_{}_md_{}.zip\".format(i,j), choice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9ZHdUFe4v-V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ZtfDgWX_b2KO",
    "lr7Sgk90kyfm",
    "1e-IJMNFdUsA"
   ],
   "machine_shape": "hm",
   "name": "Pest Recognition Full Training",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
